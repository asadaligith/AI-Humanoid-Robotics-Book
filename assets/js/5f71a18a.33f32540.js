"use strict";(self.webpackChunkai_humanoid_robotics_book=self.webpackChunkai_humanoid_robotics_book||[]).push([[6577],{1964:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>d,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"modules/module-04-vla-vision-language-action/chapter-01-introduction","title":"Chapter 1: Introduction to VLA","description":"Understand Vision-Language-Action models for embodied AI","source":"@site/docs/modules/module-04-vla-vision-language-action/chapter-01-introduction.md","sourceDirName":"modules/module-04-vla-vision-language-action","slug":"/modules/module-04-vla-vision-language-action/chapter-01-introduction","permalink":"/docs/modules/module-04-vla-vision-language-action/chapter-01-introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/asadaligith/AI-Humanoid-Robotics-Book/tree/main/docs/modules/module-04-vla-vision-language-action/chapter-01-introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Chapter 1: Introduction to VLA","description":"Understand Vision-Language-Action models for embodied AI","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/docs/modules/module-04-vla-vision-language-action/"},"next":{"title":"2\ufe0f\u20e3 Speech Recognition (Whisper)","permalink":"/docs/modules/module-04-vla-vision-language-action/chapter-02-whisper"}}');var o=t(4848),r=t(8453);const s={title:"Chapter 1: Introduction to VLA",description:"Understand Vision-Language-Action models for embodied AI",sidebar_position:1},d="Chapter 1: Introduction to VLA",a={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"What is VLA?",id:"what-is-vla",level:2},{value:"Modular vs. End-to-End",id:"modular-vs-end-to-end",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-1-introduction-to-vla",children:"Chapter 1: Introduction to VLA"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Define"})," Vision-Language-Action (VLA) models"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Explain"})," the role of LLMs in robotics"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Compare"})," VLA approaches (end-to-end vs. modular)"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"what-is-vla",children:"What is VLA?"}),"\n",(0,o.jsx)(n.p,{children:"VLA models combine:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Vision"}),": Perceive environment (cameras, depth sensors)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Language"}),": Understand commands (NLP, LLMs)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action"}),": Execute tasks (robot control)"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Example Workflow"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'User: "Pick up the red cube"\r\n  \u2192 Whisper: Transcribe audio to text\r\n  \u2192 LLM: Generate action plan JSON\r\n  \u2192 ROS Executor: Navigate, grasp, lift\n'})}),"\n",(0,o.jsx)(n.h2,{id:"modular-vs-end-to-end",children:"Modular vs. End-to-End"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Approach"}),(0,o.jsx)(n.th,{children:"Architecture"}),(0,o.jsx)(n.th,{children:"Pros"}),(0,o.jsx)(n.th,{children:"Cons"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Modular"})}),(0,o.jsx)(n.td,{children:"Whisper + LLM + ROS"}),(0,o.jsx)(n.td,{children:"Interpretable, debuggable"}),(0,o.jsx)(n.td,{children:"Complex integration"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"End-to-End"})}),(0,o.jsx)(n.td,{children:"Single neural network"}),(0,o.jsx)(n.td,{children:"Simpler"}),(0,o.jsx)(n.td,{children:"Black box, needs massive data"})]})]})]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"This course"}),": Modular approach (industry standard)"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Next"}),": ",(0,o.jsx)(n.a,{href:"/docs/modules/module-04-vla-vision-language-action/chapter-02-whisper",children:"Chapter 2: Speech Recognition with Whisper"})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Reading Time"}),": 15 minutes"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>d});var i=t(6540);const o={},r=i.createContext(o);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);