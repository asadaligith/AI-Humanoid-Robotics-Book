"use strict";(self.webpackChunkai_humanoid_robotics_book=self.webpackChunkai_humanoid_robotics_book||[]).push([[2914],{344:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"modules/module-04-vla-vision-language-action/chapter-03-llm-planning","title":"Chapter 3: LLM Task Planning","description":"Use Claude/GPT-4 to generate robot action plans from natural language","source":"@site/docs/modules/module-04-vla-vision-language-action/chapter-03-llm-planning.md","sourceDirName":"modules/module-04-vla-vision-language-action","slug":"/modules/module-04-vla-vision-language-action/chapter-03-llm-planning","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-04-vla-vision-language-action/chapter-03-llm-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/asadaligith/AI-Humanoid-Robotics-Book/tree/main/docs/modules/module-04-vla-vision-language-action/chapter-03-llm-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Chapter 3: LLM Task Planning","description":"Use Claude/GPT-4 to generate robot action plans from natural language","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"2\ufe0f\u20e3 Speech Recognition (Whisper)","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-04-vla-vision-language-action/chapter-02-whisper"},"next":{"title":"4\ufe0f\u20e3 ROS Action Executor","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-04-vla-vision-language-action/chapter-04-ros-executor"}}');var a=o(4848),i=o(8453);const s={title:"Chapter 3: LLM Task Planning",description:"Use Claude/GPT-4 to generate robot action plans from natural language",sidebar_position:3},r="Chapter 3: LLM Task Planning",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"System Prompt Design",id:"system-prompt-design",level:2},{value:"Anthropic Claude Integration",id:"anthropic-claude-integration",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-3-llm-task-planning",children:"Chapter 3: LLM Task Planning"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Design"})," prompts for robot task planning"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Parse"})," LLM outputs into structured action plans"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Handle"})," failure cases and ambiguous commands"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"system-prompt-design",children:"System Prompt Design"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'SYSTEM_PROMPT = """\nYou are a robot task planner. Convert user commands into JSON action plans.\n\nAvailable actions:\n- navigate_to(x, y, theta)\n- pick_object(object_id)\n- place_object(x, y, z)\n\nExample:\nUser: "Pick up the red cube"\nOutput: {"actions": [\n  {"type": "navigate_to", "target": "red_cube"},\n  {"type": "pick_object", "object_id": "red_cube"}\n]}\n"""\n'})}),"\n",(0,a.jsx)(n.h2,{id:"anthropic-claude-integration",children:"Anthropic Claude Integration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import anthropic\n\nclient = anthropic.Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])\nresponse = client.messages.create(\n    model="claude-3-5-sonnet-20241022",\n    max_tokens=1024,\n    system=SYSTEM_PROMPT,\n    messages=[{"role": "user", "content": "Pick up the red cube"}]\n)\nplan = json.loads(response.content[0].text)\n'})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Code Example"}),": See ",(0,a.jsx)(n.code,{children:"examples/module-04-vla/example-01-vla-pipeline/llm_planner.py"})]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Next"}),": ",(0,a.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-04-vla-vision-language-action/chapter-04-ros-executor",children:"Chapter 4: ROS Action Executor"})]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Reading Time"}),": 20 minutes\n",(0,a.jsx)(n.strong,{children:"Hands-On Time"}),": 45 minutes"]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>r});var t=o(6540);const a={},i=t.createContext(a);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);