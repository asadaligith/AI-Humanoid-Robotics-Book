"use strict";(self.webpackChunkai_humanoid_robotics_book=self.webpackChunkai_humanoid_robotics_book||[]).push([[2914],{344:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"modules/module-04-vla-vision-language-action/chapter-03-llm-planning","title":"Chapter 3: LLM Task Planning","description":"Use Claude/GPT-4 to generate robot action plans from natural language","source":"@site/docs/modules/module-04-vla-vision-language-action/chapter-03-llm-planning.md","sourceDirName":"modules/module-04-vla-vision-language-action","slug":"/modules/module-04-vla-vision-language-action/chapter-03-llm-planning","permalink":"/docs/modules/module-04-vla-vision-language-action/chapter-03-llm-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/asadaligith/AI-Humanoid-Robotics-Book/tree/main/docs/modules/module-04-vla-vision-language-action/chapter-03-llm-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Chapter 3: LLM Task Planning","description":"Use Claude/GPT-4 to generate robot action plans from natural language","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"2\ufe0f\u20e3 Speech Recognition (Whisper)","permalink":"/docs/modules/module-04-vla-vision-language-action/chapter-02-whisper"},"next":{"title":"4\ufe0f\u20e3 ROS Action Executor","permalink":"/docs/modules/module-04-vla-vision-language-action/chapter-04-ros-executor"}}');var o=t(4848),r=t(8453);const i={title:"Chapter 3: LLM Task Planning",description:"Use Claude/GPT-4 to generate robot action plans from natural language",sidebar_position:3},s="Chapter 3: LLM Task Planning",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"System Prompt Design",id:"system-prompt-design",level:2},{value:"Anthropic Claude Integration",id:"anthropic-claude-integration",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-3-llm-task-planning",children:"Chapter 3: LLM Task Planning"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Design"})," prompts for robot task planning"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parse"})," LLM outputs into structured action plans"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Handle"})," failure cases and ambiguous commands"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"system-prompt-design",children:"System Prompt Design"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'SYSTEM_PROMPT = """\r\nYou are a robot task planner. Convert user commands into JSON action plans.\r\n\r\nAvailable actions:\r\n- navigate_to(x, y, theta)\r\n- pick_object(object_id)\r\n- place_object(x, y, z)\r\n\r\nExample:\r\nUser: "Pick up the red cube"\r\nOutput: {"actions": [\r\n  {"type": "navigate_to", "target": "red_cube"},\r\n  {"type": "pick_object", "object_id": "red_cube"}\r\n]}\r\n"""\n'})}),"\n",(0,o.jsx)(n.h2,{id:"anthropic-claude-integration",children:"Anthropic Claude Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import anthropic\r\n\r\nclient = anthropic.Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])\r\nresponse = client.messages.create(\r\n    model="claude-3-5-sonnet-20241022",\r\n    max_tokens=1024,\r\n    system=SYSTEM_PROMPT,\r\n    messages=[{"role": "user", "content": "Pick up the red cube"}]\r\n)\r\nplan = json.loads(response.content[0].text)\n'})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Code Example"}),": See ",(0,o.jsx)(n.code,{children:"examples/module-04-vla/example-01-vla-pipeline/llm_planner.py"})]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Next"}),": ",(0,o.jsx)(n.a,{href:"/docs/modules/module-04-vla-vision-language-action/chapter-04-ros-executor",children:"Chapter 4: ROS Action Executor"})]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Reading Time"}),": 20 minutes\r\n",(0,o.jsx)(n.strong,{children:"Hands-On Time"}),": 45 minutes"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>s});var a=t(6540);const o={},r=a.createContext(o);function i(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);