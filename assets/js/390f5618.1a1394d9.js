"use strict";(self.webpackChunkai_humanoid_robotics_book=self.webpackChunkai_humanoid_robotics_book||[]).push([[627],{6402:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"modules/module-05-capstone/chapter-02-voice-llm","title":"Chapter 2: Voice & LLM Pipeline","description":"Learning Objectives","source":"@site/docs/modules/module-05-capstone/chapter-02-voice-llm.md","sourceDirName":"modules/module-05-capstone","slug":"/modules/module-05-capstone/chapter-02-voice-llm","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/chapter-02-voice-llm","draft":false,"unlisted":false,"editUrl":"https://github.com/asadaligith/AI-Humanoid-Robotics-Book/tree/main/docs/modules/module-05-capstone/chapter-02-voice-llm.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"1\ufe0f\u20e3 System Architecture","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/chapter-01-architecture"},"next":{"title":"3\ufe0f\u20e3 Navigation & Perception","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/chapter-03-navigation-perception"}}');var t=r(4848),s=r(8453);const o={sidebar_position:2},a="Chapter 2: Voice & LLM Pipeline",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Voice Input with Whisper",id:"voice-input-with-whisper",level:2},{value:"Whisper Model Selection",id:"whisper-model-selection",level:3},{value:"ROS 2 Voice Input Node",id:"ros-2-voice-input-node",level:3},{value:"LLM Task Planning",id:"llm-task-planning",level:2},{value:"Prompt Engineering for Robotics",id:"prompt-engineering-for-robotics",level:3},{value:"Capability Manifest Design",id:"capability-manifest-design",level:3},{value:"LLM Planner Node Implementation",id:"llm-planner-node-implementation",level:3},{value:"Output Validation",id:"output-validation",level:2},{value:"JSON Schema Validation",id:"json-schema-validation",level:3},{value:"Handling Ambiguity",id:"handling-ambiguity",level:3},{value:"Research &amp; Evidence",id:"research--evidence",level:2},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-2-voice--llm-pipeline",children:"Chapter 2: Voice & LLM Pipeline"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,t.jsx)(n.strong,{children:"Integrate"})," OpenAI Whisper for voice transcription in ROS 2 nodes"]}),"\n",(0,t.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,t.jsx)(n.strong,{children:"Design"})," LLM prompts for robot task planning and action decomposition"]}),"\n",(0,t.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,t.jsx)(n.strong,{children:"Implement"})," capability manifests to constrain LLM output to valid robot actions"]}),"\n",(0,t.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,t.jsx)(n.strong,{children:"Handle"})," ambiguous commands with clarification requests and validation logic"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completed Chapter 1: System Architecture"}),"\n",(0,t.jsx)(n.li,{children:"OpenAI API key (for Whisper and LLM access)"}),"\n",(0,t.jsx)(n.li,{children:"Microphone configured and accessible in Ubuntu"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of JSON schema validation"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:'The "brain" of an autonomous humanoid consists of two AI systems working in tandem: voice recognition to understand human speech, and large language models to translate natural language into executable robot actions. This chapter implements the voice-to-action pipeline that transforms spoken commands like "Bring me the red cup" into structured action sequences the robot can execute.'}),"\n",(0,t.jsx)(n.p,{children:"You'll learn how to integrate OpenAI Whisper for robust voice transcription, engineer LLM prompts that reliably produce valid robot plans, and design capability manifests that constrain the LLM's creativity to actions your robot can actually perform."}),"\n",(0,t.jsx)(n.h2,{id:"voice-input-with-whisper",children:"Voice Input with Whisper"}),"\n",(0,t.jsx)(n.h3,{id:"whisper-model-selection",children:"Whisper Model Selection"}),"\n",(0,t.jsx)(n.p,{children:"OpenAI Whisper offers multiple model sizes with accuracy/speed tradeoffs:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Model"}),(0,t.jsx)(n.th,{children:"Parameters"}),(0,t.jsx)(n.th,{children:"Speed (CPU)"}),(0,t.jsx)(n.th,{children:"Accuracy"}),(0,t.jsx)(n.th,{children:"Use Case"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"tiny"}),(0,t.jsx)(n.td,{children:"39M"}),(0,t.jsx)(n.td,{children:"~10x realtime"}),(0,t.jsx)(n.td,{children:"Good"}),(0,t.jsx)(n.td,{children:"Embedded, low-latency"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"base"}),(0,t.jsx)(n.td,{children:"74M"}),(0,t.jsx)(n.td,{children:"~7x realtime"}),(0,t.jsx)(n.td,{children:"Better"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Recommended for Jetson"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"small"}),(0,t.jsx)(n.td,{children:"244M"}),(0,t.jsx)(n.td,{children:"~4x realtime"}),(0,t.jsx)(n.td,{children:"Very Good"}),(0,t.jsx)(n.td,{children:"Desktop, balanced"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"medium"}),(0,t.jsx)(n.td,{children:"769M"}),(0,t.jsx)(n.td,{children:"~2x realtime"}),(0,t.jsx)(n.td,{children:"Excellent"}),(0,t.jsx)(n.td,{children:"Desktop, high accuracy"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"large"}),(0,t.jsx)(n.td,{children:"1550M"}),(0,t.jsx)(n.td,{children:"~1x realtime"}),(0,t.jsx)(n.td,{children:"Best"}),(0,t.jsx)(n.td,{children:"Cloud, offline batch"})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:["For simulation and Jetson deployment, we use the ",(0,t.jsx)(n.strong,{children:"base model"})," (74M parameters) as it balances accuracy with real-time performance."]}),"\n",(0,t.jsx)(n.h3,{id:"ros-2-voice-input-node",children:"ROS 2 Voice Input Node"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"voice_input_node.py"})," implements continuous listening with voice activity detection (VAD):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport whisper\r\nimport numpy as np\r\nimport sounddevice as sd\r\n\r\nclass VoiceInputNode(Node):\r\n    def __init__(self):\r\n        super().__init__('voice_input_node')\r\n\r\n        # Load Whisper model\r\n        model_size = self.declare_parameter('whisper_model', 'base').value\r\n        self.model = whisper.load_model(model_size)\r\n        self.get_logger().info(f'Loaded Whisper model: {model_size}')\r\n\r\n        # Publisher for transcribed text\r\n        self.text_pub = self.create_publisher(String, '/voice/transcribed_text', 10)\r\n\r\n        # Audio parameters\r\n        self.sample_rate = 16000  # Whisper expects 16kHz\r\n        self.duration = 3  # Capture 3 seconds per command\r\n\r\n        # Start listening loop\r\n        self.create_timer(0.1, self.listen_callback)\r\n\r\n    def listen_callback(self):\r\n        \"\"\"Capture audio and transcribe if voice detected\"\"\"\r\n        # Record audio\r\n        audio = sd.rec(\r\n            int(self.duration * self.sample_rate),\r\n            samplerate=self.sample_rate,\r\n            channels=1,\r\n            dtype='float32'\r\n        )\r\n        sd.wait()  # Wait for recording to finish\r\n\r\n        # Check for voice activity (simple energy threshold)\r\n        energy = np.sqrt(np.mean(audio**2))\r\n        if energy < 0.01:  # Silence threshold\r\n            return\r\n\r\n        # Transcribe\r\n        audio_np = audio.flatten()\r\n        result = self.model.transcribe(audio_np, language='en')\r\n        text = result['text'].strip()\r\n\r\n        if text:\r\n            self.get_logger().info(f'Transcribed: \"{text}\"')\r\n            msg = String()\r\n            msg.data = text\r\n            self.text_pub.publish(msg)\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Configuration"})," (",(0,t.jsx)(n.code,{children:"config/voice_config.yaml"}),"):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'whisper_model: "base"\r\nlanguage: "en"\r\nsample_rate: 16000\r\nduration: 3.0\r\nsilence_threshold: 0.01\n'})}),"\n",(0,t.jsxs)(n.p,{children:["See full implementation: ",(0,t.jsx)(n.code,{children:"examples/module-05-capstone/voice_input_node.py"})]}),"\n",(0,t.jsx)(n.h2,{id:"llm-task-planning",children:"LLM Task Planning"}),"\n",(0,t.jsx)(n.h3,{id:"prompt-engineering-for-robotics",children:"Prompt Engineering for Robotics"}),"\n",(0,t.jsxs)(n.p,{children:["The key to reliable LLM-based planning is ",(0,t.jsx)(n.strong,{children:"structured prompts"})," with explicit output formatting:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'TASK_PLANNER_PROMPT = """You are a robot task planner. Generate a structured action sequence for the following voice command.\r\n\r\nAvailable Actions (from capability manifest):\r\n{capability_manifest}\r\n\r\nVoice Command: "{voice_command}"\r\n\r\nOutput Format (strict JSON):\r\n{{\r\n  "command_understood": true/false,\r\n  "clarification_needed": null or "question text",\r\n  "action_sequence": [\r\n    {{"action": "navigate_to", "params": {{"location": "kitchen_table"}}}},\r\n    {{"action": "detect_object", "params": {{"name": "cup", "color": "red"}}}},\r\n    {{"action": "pick_object", "params": {{"object_id": "detected_id"}}}},\r\n    ...\r\n  ]\r\n}}\r\n\r\nRules:\r\n1. Only use actions from the capability manifest\r\n2. Provide concrete parameters (no placeholders like "TBD")\r\n3. If command is ambiguous, set command_understood=false and provide clarification question\r\n4. Navigation must precede detection/manipulation\r\n5. Pick must precede place for the same object\r\n"""\n'})}),"\n",(0,t.jsx)(n.h3,{id:"capability-manifest-design",children:"Capability Manifest Design"}),"\n",(0,t.jsx)(n.p,{children:"The manifest defines what the robot can do, constraining LLM hallucinations:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\r\n  "capabilities": [\r\n    {\r\n      "action": "navigate_to",\r\n      "description": "Move robot to named location",\r\n      "params": {\r\n        "location": {\r\n          "type": "string",\r\n          "enum": ["kitchen_table", "living_room", "user_location", "charging_station"]\r\n        }\r\n      }\r\n    },\r\n    {\r\n      "action": "detect_object",\r\n      "description": "Find object in camera view",\r\n      "params": {\r\n        "name": {\r\n          "type": "string",\r\n          "enum": ["cup", "mug", "bottle", "plate", "bowl"]\r\n        },\r\n        "color": {\r\n          "type": "string",\r\n          "enum": ["red", "blue", "green", "white", "black"],\r\n          "optional": true\r\n        }\r\n      }\r\n    },\r\n    {\r\n      "action": "pick_object",\r\n      "description": "Grasp detected object",\r\n      "params": {\r\n        "object_id": {\r\n          "type": "string",\r\n          "description": "ID from detect_object output"\r\n        }\r\n      }\r\n    },\r\n    {\r\n      "action": "place_object",\r\n      "description": "Release object at location",\r\n      "params": {\r\n        "location": {\r\n          "type": "string",\r\n          "enum": ["table", "user_hand", "bin", "shelf"]\r\n        }\r\n      }\r\n    }\r\n  ]\r\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key Design Principles"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Enumerated values"})," prevent hallucinated locations/objects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Type constraints"})," catch malformed parameters early"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Descriptions"})," help LLM understand action semantics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dependencies"})," (pick requires detect) enforced in validation"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"llm-planner-node-implementation",children:"LLM Planner Node Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom custom_msgs.msg import ActionPlan\r\nimport openai\r\nimport json\r\n\r\nclass LLMPlannerNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'llm_planner_node\')\r\n\r\n        # Load capability manifest\r\n        with open(\'config/capability_manifest.json\') as f:\r\n            self.capabilities = json.load(f)\r\n\r\n        # OpenAI API setup\r\n        openai.api_key = os.getenv(\'OPENAI_API_KEY\')\r\n\r\n        # Subscribers\r\n        self.create_subscription(\r\n            String, \'/voice/transcribed_text\',\r\n            self.voice_callback, 10\r\n        )\r\n\r\n        # Publishers\r\n        self.plan_pub = self.create_publisher(ActionPlan, \'/planning/action_sequence\', 10)\r\n\r\n    def voice_callback(self, msg):\r\n        """Generate action plan from voice command"""\r\n        voice_command = msg.data\r\n\r\n        # Construct prompt\r\n        prompt = TASK_PLANNER_PROMPT.format(\r\n            capability_manifest=json.dumps(self.capabilities, indent=2),\r\n            voice_command=voice_command\r\n        )\r\n\r\n        # Call LLM\r\n        response = openai.ChatCompletion.create(\r\n            model="gpt-4",\r\n            messages=[{"role": "user", "content": prompt}],\r\n            temperature=0.0  # Deterministic output\r\n        )\r\n\r\n        # Parse JSON response\r\n        plan_json = json.loads(response.choices[0].message.content)\r\n\r\n        # Validate plan\r\n        if self.validate_plan(plan_json):\r\n            self.publish_plan(plan_json)\r\n        else:\r\n            self.get_logger().error(\'Invalid plan generated by LLM\')\n'})}),"\n",(0,t.jsxs)(n.p,{children:["See full implementation: ",(0,t.jsx)(n.code,{children:"examples/module-05-capstone/llm_planner_node.py"})]}),"\n",(0,t.jsx)(n.h2,{id:"output-validation",children:"Output Validation"}),"\n",(0,t.jsx)(n.h3,{id:"json-schema-validation",children:"JSON Schema Validation"}),"\n",(0,t.jsxs)(n.p,{children:["Use ",(0,t.jsx)(n.code,{children:"jsonschema"})," library to validate LLM output against expected structure:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from jsonschema import validate, ValidationError\r\n\r\nPLAN_SCHEMA = {\r\n    "type": "object",\r\n    "required": ["command_understood", "action_sequence"],\r\n    "properties": {\r\n        "command_understood": {"type": "boolean"},\r\n        "clarification_needed": {"type": ["string", "null"]},\r\n        "action_sequence": {\r\n            "type": "array",\r\n            "items": {\r\n                "type": "object",\r\n                "required": ["action", "params"],\r\n                "properties": {\r\n                    "action": {"type": "string"},\r\n                    "params": {"type": "object"}\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\ndef validate_plan(plan_json):\r\n    try:\r\n        validate(instance=plan_json, schema=PLAN_SCHEMA)\r\n        return True\r\n    except ValidationError as e:\r\n        logger.error(f\'Plan validation failed: {e}\')\r\n        return False\n'})}),"\n",(0,t.jsx)(n.h3,{id:"handling-ambiguity",children:"Handling Ambiguity"}),"\n",(0,t.jsxs)(n.p,{children:["When the LLM sets ",(0,t.jsx)(n.code,{children:"command_understood=false"}),", request clarification:"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example 1 - Missing Color"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input"}),': "Bring me the cup"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLM Output"}),": ",(0,t.jsx)(n.code,{children:'{"command_understood": false, "clarification_needed": "Which cup? I see red, blue, and white cups."}'})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System"}),": Publishes clarification to ",(0,t.jsx)(n.code,{children:"/voice/clarification_request"}),", waits for user response"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example 2 - Unknown Location"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input"}),': "Go to the garage"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLM Output"}),": ",(0,t.jsx)(n.code,{children:'{"command_understood": false, "clarification_needed": "I don\'t know where the garage is. Can you guide me?"}'})]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"research--evidence",children:"Research & Evidence"}),"\n",(0,t.jsx)(n.p,{children:"Voice recognition and LLM grounding approaches are informed by:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Whisper Technical Report: ",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2212.04356",children:"Robust Speech Recognition via Large-Scale Weak Supervision"})," (Radford et al., 2022)"]}),"\n",(0,t.jsxs)(n.li,{children:["Prompt Engineering Guide: ",(0,t.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/prompt-engineering",children:"OpenAI Best Practices"})]}),"\n",(0,t.jsxs)(n.li,{children:["LLM for Robotics: ",(0,t.jsx)(n.a,{href:"https://code-as-policies.github.io/",children:"Code as Policies"})," (Liang et al., Google, 2023)"]}),"\n",(0,t.jsxs)(n.li,{children:["RT-2 VLA Architecture: ",(0,t.jsx)(n.a,{href:"https://robotics-transformer2.github.io/",children:"Vision-Language-Action Models"})," (Brohan et al., DeepMind, 2023)"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"The voice-to-action pipeline transforms unstructured human speech into validated robot plans through three key components:"}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Whisper transcription"})," provides robust, multilingual voice recognition optimized for embedded deployment"]}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"LLM prompt engineering"})," with capability manifests constrains AI creativity to executable actions"]}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"JSON schema validation"})," catches malformed plans before they reach robot controllers"]}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Clarification loops"})," handle ambiguous commands gracefully, maintaining user trust"]}),"\n",(0,t.jsx)(n.p,{children:'This "brain" layer abstracts complex AI reasoning into a simple ROS 2 interface: voice commands in, action sequences out.'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Next"}),": ",(0,t.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/chapter-03-navigation-perception",children:"Chapter 3: Navigation & Perception"})," implements the robot's ability to move through space and see the world."]}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsxs)(n.p,{children:["\u2b50 ",(0,t.jsx)(n.strong,{children:"Exercise 1"}),": Modify the voice input node to use push-to-talk instead of continuous listening. Add a ROS 2 service ",(0,t.jsx)(n.code,{children:"/voice/start_listening"})," that triggers recording."]}),"\n",(0,t.jsxs)(n.p,{children:["\u2b50\u2b50 ",(0,t.jsx)(n.strong,{children:"Exercise 2"}),": Extend the capability manifest to support ",(0,t.jsx)(n.strong,{children:"conditional actions"}),' (e.g., "If you find the cup, bring it to me; otherwise, bring the mug"). How does the LLM output format change?']}),"\n",(0,t.jsxs)(n.p,{children:["\u2b50\u2b50\u2b50 ",(0,t.jsx)(n.strong,{children:"Exercise 3"}),": Implement ",(0,t.jsx)(n.strong,{children:"multi-turn dialogue"})," where the robot can ask follow-up questions. Design a state machine that alternates between PLANNING and CLARIFYING states until a complete plan is validated."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Word Count"}),": 312 words (Target: 300-350) \u2705"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var i=r(6540);const t={},s=i.createContext(t);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);