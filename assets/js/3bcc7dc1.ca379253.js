"use strict";(self.webpackChunkai_humanoid_robotics_book=self.webpackChunkai_humanoid_robotics_book||[]).push([[603],{1451:(n,e,s)=>{s.r(e),s.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"appendices/glossary","title":"Glossary","description":"Technical terms and acronyms used throughout the AI Humanoid Robotics Book.","source":"@site/docs/appendices/glossary.md","sourceDirName":"appendices","slug":"/appendices/glossary","permalink":"/AI-Humanoid-Robotics-Book/docs/appendices/glossary","draft":false,"unlisted":false,"editUrl":"https://github.com/asadaligith/AI-Humanoid-Robotics-Book/tree/main/docs/appendices/glossary.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3}}');var o=s(4848),i=s(8453);const t={sidebar_position:3},a="Glossary",l={},c=[{value:"A",id:"a",level:2},{value:"B",id:"b",level:2},{value:"C",id:"c",level:2},{value:"D",id:"d",level:2},{value:"E",id:"e",level:2},{value:"F",id:"f",level:2},{value:"G",id:"g",level:2},{value:"H",id:"h",level:2},{value:"I",id:"i",level:2},{value:"J",id:"j",level:2},{value:"K",id:"k",level:2},{value:"L",id:"l",level:2},{value:"M",id:"m",level:2},{value:"N",id:"n",level:2},{value:"O",id:"o",level:2},{value:"P",id:"p",level:2},{value:"Q",id:"q",level:2},{value:"R",id:"r",level:2},{value:"S",id:"s",level:2},{value:"T",id:"t",level:2},{value:"U",id:"u",level:2},{value:"V",id:"v",level:2},{value:"W",id:"w",level:2},{value:"X",id:"x",level:2},{value:"Y",id:"y",level:2},{value:"Symbols and Abbreviations",id:"symbols-and-abbreviations",level:2},{value:"Frame Convention (ROS REP 103)",id:"frame-convention-ros-rep-103",level:2},{value:"Common ROS 2 Message Types",id:"common-ros-2-message-types",level:2},{value:"Common ROS 2 Topics",id:"common-ros-2-topics",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"glossary",children:"Glossary"})}),"\n",(0,o.jsx)(e.p,{children:"Technical terms and acronyms used throughout the AI Humanoid Robotics Book."}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"a",children:"A"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Action"})," (ROS 2)\n: A communication pattern in ROS 2 for long-running tasks with feedback. Consists of goal, feedback, and result messages. Example: Navigation to a waypoint."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Actuator"}),"\n: A component that produces motion or force. Examples: Servo motors, DC motors, pneumatic cylinders."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"APA Citation"}),"\n: American Psychological Association citation format (7th edition) used for academic references in this book."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"API (Application Programming Interface)"}),"\n: A set of functions and protocols for building software. Example: Claude API for LLM task planning."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Autonomous Navigation"}),"\n: The ability of a robot to move from point A to B without human intervention, using sensors and planning algorithms."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"b",children:"B"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Base Link"}),"\n: The primary coordinate frame attached to a robot's main body, typically at the center of rotation. All other frames are defined relative to base_link."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Bag File"})," (ROS 2)\n: A file format (.db3) for recording and replaying ROS 2 topic data. Used for debugging and testing."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"BOM (Bill of Materials)"}),"\n: A comprehensive list of components, quantities, and costs required to build a hardware system."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Bounding Box"}),"\n: A rectangular box around a detected object in an image, defined by (x, y, width, height) or corner coordinates."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Buck Converter"}),"\n: A DC-to-DC power converter that steps down voltage efficiently. Example: 14.8V battery to 5V for Jetson."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"c",children:"C"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Callback"}),"\n: A function executed in response to an event, such as receiving a message on a ROS 2 topic."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Camera Optical Frame"}),"\n: A coordinate frame aligned with a camera's optical axis, following the convention: X right, Y down, Z forward (into the scene)."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Capstone Project"}),"\n: Module 5 of this book, integrating all previous modules into an autonomous humanoid robot system."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"COCO Dataset"}),"\n: Common Objects in Context, a large-scale object detection dataset with 80 classes (20+ used in this book)."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Colcon"}),"\n: The build tool for ROS 2 workspaces, replacing catkin from ROS 1."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Context7"}),"\n: An MCP (Model Context Protocol) server for managing citations and documentation links in AI-assisted workflows."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Coordinate Frame"}),'\n: A reference system defining position and orientation, with X, Y, Z axes. Also called "coordinate system" or "frame."']}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"CUDA (Compute Unified Device Architecture)"}),"\n: NVIDIA's parallel computing platform for GPU-accelerated applications."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"d",children:"D"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"DDS (Data Distribution Service)"}),"\n: The middleware protocol used by ROS 2 for inter-process communication, providing publish-subscribe and request-response patterns."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Depth Camera"}),"\n: A camera that captures distance (depth) information in addition to RGB images. Example: Intel RealSense D435i."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Digital Twin"}),"\n: A virtual replica of a physical system used for simulation and testing before real-world deployment."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Docker"}),"\n: A containerization platform for packaging software with dependencies into isolated, reproducible environments."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"DOF (Degrees of Freedom)"}),"\n: The number of independent ways a system can move. A 6-DOF robot arm has 6 joints allowing 6 independent motions."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Dynamixel"}),"\n: A brand of smart servo motors by ROBOTIS, commonly used in robotics for precise position control with feedback."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"e",children:"E"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Embodied AI"}),"\n: Artificial intelligence deployed in physical agents (robots) that interact with the real world, as opposed to purely digital AI."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"End Effector"}),"\n: The device at the end of a robotic arm (e.g., gripper, suction cup) that interacts with objects."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Executor"})," (ROS 2)\n: A component that manages callback execution for ROS 2 nodes, handling subscriptions, timers, and services."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"f",children:"F"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Forward Kinematics"}),"\n: Computing the position and orientation of a robot's end effector from its joint angles."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"FPS (Frames Per Second)"}),"\n: The rate at which images are captured or processed, critical for real-time perception systems."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Frame ID"}),'\n: A string identifier for a coordinate frame in ROS 2 TF2 system. Example: "base_link", "camera_optical_frame".']}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"g",children:"G"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Gazebo"}),"\n: An open-source 3D robotics simulator, successor to Gazebo Classic. Used for testing robots in virtual environments."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"GitHub Actions"}),"\n: A CI/CD platform for automating workflows like testing and deployment directly from GitHub repositories."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Grasp Pose"}),"\n: The position and orientation a robot gripper must achieve to successfully grasp an object."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"GPU (Graphics Processing Unit)"}),"\n: Specialized hardware for parallel computations, essential for AI inference and simulation rendering."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"h",children:"H"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Homogeneous Transformation"}),"\n: A 4x4 matrix representing both rotation and translation in 3D space, used in robotics for coordinate frame transformations."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Humanoid Robot"}),"\n: A robot with a human-like body structure, typically including head, torso, arms, and legs."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"i",children:"I"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"IMU (Inertial Measurement Unit)"}),"\n: A sensor measuring acceleration and angular velocity, used for odometry and stabilization."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Inverse Kinematics"}),"\n: Computing the joint angles required to position a robot's end effector at a desired location (opposite of forward kinematics)."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Isaac Sim"}),"\n: NVIDIA's photorealistic robotics simulator built on Omniverse, supporting GPU-accelerated physics and rendering."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"j",children:"J"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Jetson"}),"\n: NVIDIA's embedded computing platform for edge AI, commonly used in autonomous robots. Example: Jetson Orin Nano."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Joint State"}),"\n: The position, velocity, and effort (torque) of a robot's joints, published on ",(0,o.jsx)(e.code,{children:"/joint_states"})," topic in ROS 2."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"k",children:"K"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Kinematics"}),"\n: The study of motion without considering forces. In robotics: forward and inverse kinematics for arm movement."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"l",children:"L"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Launch File"}),"\n: A Python or XML file in ROS 2 that starts multiple nodes with configuration parameters simultaneously."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"LiDAR (Light Detection and Ranging)"}),"\n: A sensor that measures distances using laser beams, creating 2D or 3D maps of the environment."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"LLM (Large Language Model)"}),"\n: A neural network trained on text for natural language understanding and generation. Example: Anthropic Claude."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"LTS (Long-Term Support)"}),"\n: A software release maintained with updates for extended periods. Example: Ubuntu 22.04 LTS, ROS 2 Humble (5 years)."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"m",children:"M"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Manipulation"}),"\n: The task of grasping, moving, and placing objects using a robotic arm."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"MCP (Model Context Protocol)"}),"\n: A protocol for connecting AI assistants to external data sources, used for Context7 citation management."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Message"})," (ROS 2)\n: A data structure transmitted over topics. Example: ",(0,o.jsx)(e.code,{children:"geometry_msgs/Twist"})," for velocity commands."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Model (ML)"}),"\n: A trained neural network for tasks like object detection, segmentation, or voice recognition."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"n",children:"N"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Nav2 (Navigation2)"}),"\n: The ROS 2 navigation framework for autonomous path planning and obstacle avoidance."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Node"})," (ROS 2)\n: An executable program that performs computation and communicates via topics, services, or actions."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"NVIDIA Container Toolkit"}),"\n: Software enabling GPU access inside Docker containers for CUDA applications."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"o",children:"O"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Odometry"}),"\n: Estimating a robot's position and velocity from sensor data (wheel encoders, IMU, visual odometry)."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Omniverse"}),"\n: NVIDIA's platform for 3D collaboration and simulation, hosting Isaac Sim."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"p",children:"P"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Package"})," (ROS 2)\n: A directory containing nodes, launch files, configuration, and metadata (package.xml) organized as a unit."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Perception"}),"\n: The robotic capability to interpret sensor data (cameras, LiDAR) to understand the environment."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Physical AI"}),"\n: AI systems that interact with the physical world through robotic embodiment (sensors, actuators)."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Pose"}),"\n: Position (x, y, z) and orientation (quaternion or Euler angles) of an object or robot in 3D space."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Publisher"})," (ROS 2)\n: A node or component that sends messages to a topic."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"q",children:"Q"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"QoS (Quality of Service)"}),"\n: Configuration for ROS 2 communication reliability, durability, and history. Example: RELIABLE vs BEST_EFFORT."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Quaternion"}),"\n: A mathematical representation of 3D rotation using 4 values (x, y, z, w), avoiding gimbal lock."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"r",children:"R"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"RealSense"}),"\n: Intel's brand of depth cameras used for 3D perception and SLAM."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"REP (ROS Enhancement Proposal)"}),"\n: Standards documents for ROS conventions. Example: REP 103 defines coordinate frame conventions."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"RGB-D"}),"\n: RGB (color) + Depth camera, providing both visual and distance information."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Robot State Publisher"}),"\n: A ROS 2 node that publishes the robot's TF transforms based on URDF and joint states."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"ROS 2 (Robot Operating System 2)"}),"\n: An open-source framework for robot software development, successor to ROS 1."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"rqt"}),"\n: A Qt-based GUI framework for ROS 2, providing tools like topic visualization and parameter tuning."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"RViz2"}),"\n: The 3D visualization tool for ROS 2, displaying sensor data, TF frames, and robot models."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"s",children:"S"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Sensor Fusion"}),"\n: Combining data from multiple sensors (camera, LiDAR, IMU) to improve perception accuracy."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Service"})," (ROS 2)\n: A synchronous request-response communication pattern. Example: Trigger a calculation and wait for result."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"SLAM (Simultaneous Localization and Mapping)"}),"\n: Building a map while tracking the robot's position within it."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Subscriber"})," (ROS 2)\n: A node or component that receives messages from a topic."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"t",children:"T"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Task Planning"}),'\n: Decomposing high-level goals (e.g., "fetch coffee") into low-level actions (navigate, grasp, return).']}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"TF2 (Transform Library 2)"}),"\n: ROS 2 system for tracking coordinate frame relationships over time."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Topic"})," (ROS 2)\n: A named communication channel for publishing and subscribing to messages asynchronously."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Trajectory"}),"\n: A time-parameterized path defining position and velocity at each timestep for a robot's motion."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"u",children:"U"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"URDF (Unified Robot Description Format)"}),"\n: An XML format for describing robot geometry, kinematics, and dynamics in ROS."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"USD (Universal Scene Description)"}),"\n: Pixar's file format for 3D scenes, used by Isaac Sim for robot and environment models."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"v",children:"V"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"VLA (Vision-Language-Action)"}),"\n: An AI architecture combining vision (cameras), language understanding (LLMs), and action (motor commands)."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"VRAM (Video RAM)"}),"\n: Memory on a GPU for storing textures and computation data. Example: RTX 3060 has 6GB VRAM."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"VSLAM (Visual SLAM)"}),"\n: SLAM using camera images instead of LiDAR for mapping and localization."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"w",children:"W"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Whisper"}),"\n: OpenAI's neural network for speech-to-text transcription, supporting 99 languages."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Workspace"})," (ROS 2)\n: A directory structure (src/, build/, install/) containing ROS 2 packages for development."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"WSL2 (Windows Subsystem for Linux 2)"}),"\n: A compatibility layer for running Linux on Windows, useful for ROS 2 development on Windows machines."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"x",children:"X"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"XML (eXtensible Markup Language)"}),"\n: A markup language used for URDF robot descriptions, launch files (ROS 2), and configuration."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"y",children:"Y"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"YAML (YAML Ain't Markup Language)"}),"\n: A human-readable data serialization format used for ROS 2 configuration files."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"YOLO (You Only Look Once)"}),"\n: A family of real-time object detection models. Example: YOLOv8 for detecting COCO classes."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"symbols-and-abbreviations",children:"Symbols and Abbreviations"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"6-DOF"}),": 6 Degrees of Freedom\n",(0,o.jsx)(e.strong,{children:"API"}),": Application Programming Interface\n",(0,o.jsx)(e.strong,{children:"CI/CD"}),": Continuous Integration / Continuous Deployment\n",(0,o.jsx)(e.strong,{children:"CPU"}),": Central Processing Unit\n",(0,o.jsx)(e.strong,{children:"DDS"}),": Data Distribution Service\n",(0,o.jsx)(e.strong,{children:"DOF"}),": Degrees of Freedom\n",(0,o.jsx)(e.strong,{children:"FPS"}),": Frames Per Second\n",(0,o.jsx)(e.strong,{children:"GPU"}),": Graphics Processing Unit\n",(0,o.jsx)(e.strong,{children:"GUI"}),": Graphical User Interface\n",(0,o.jsx)(e.strong,{children:"I/O"}),": Input/Output\n",(0,o.jsx)(e.strong,{children:"IMU"}),": Inertial Measurement Unit\n",(0,o.jsx)(e.strong,{children:"LiDAR"}),": Light Detection and Ranging\n",(0,o.jsx)(e.strong,{children:"LLM"}),": Large Language Model\n",(0,o.jsx)(e.strong,{children:"LTS"}),": Long-Term Support\n",(0,o.jsx)(e.strong,{children:"MCP"}),": Model Context Protocol\n",(0,o.jsx)(e.strong,{children:"ML"}),": Machine Learning\n",(0,o.jsx)(e.strong,{children:"QoS"}),": Quality of Service\n",(0,o.jsx)(e.strong,{children:"RGB"}),": Red Green Blue\n",(0,o.jsx)(e.strong,{children:"RGB-D"}),": RGB + Depth\n",(0,o.jsx)(e.strong,{children:"ROS"}),": Robot Operating System\n",(0,o.jsx)(e.strong,{children:"SLAM"}),": Simultaneous Localization and Mapping\n",(0,o.jsx)(e.strong,{children:"TF"}),": Transform (ROS coordinate frame system)\n",(0,o.jsx)(e.strong,{children:"URDF"}),": Unified Robot Description Format\n",(0,o.jsx)(e.strong,{children:"USD"}),": Universal Scene Description\n",(0,o.jsx)(e.strong,{children:"VLA"}),": Vision-Language-Action\n",(0,o.jsx)(e.strong,{children:"VRAM"}),": Video RAM\n",(0,o.jsx)(e.strong,{children:"VSLAM"}),": Visual SLAM\n",(0,o.jsx)(e.strong,{children:"WSL"}),": Windows Subsystem for Linux\n",(0,o.jsx)(e.strong,{children:"YAML"}),": YAML Ain't Markup Language\n",(0,o.jsx)(e.strong,{children:"YOLO"}),": You Only Look Once"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"frame-convention-ros-rep-103",children:"Frame Convention (ROS REP 103)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Right-Hand Rule"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"X"}),": Forward (red)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Y"}),": Left (green)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Z"}),": Up (blue)"]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Rotations"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Roll"}),": Rotation around X-axis"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Pitch"}),": Rotation around Y-axis"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Yaw"}),": Rotation around Z-axis"]}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"common-ros-2-message-types",children:"Common ROS 2 Message Types"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"geometry_msgs/Twist"}),"\n: Linear and angular velocity commands for mobile robot control."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"sensor_msgs/Image"}),"\n: Camera image data (RGB, grayscale, or depth)."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"sensor_msgs/PointCloud2"}),"\n: 3D point cloud from LiDAR or depth camera."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"nav_msgs/Odometry"}),"\n: Robot pose and velocity estimates from odometry sources."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"std_msgs/String"}),"\n: Simple string message for text data."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"tf2_msgs/TFMessage"}),"\n: Transform data for coordinate frame relationships."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"common-ros-2-topics",children:"Common ROS 2 Topics"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"/cmd_vel"}),": Velocity commands (geometry_msgs/Twist)\n",(0,o.jsx)(e.strong,{children:"/joint_states"}),": Robot joint positions and velocities\n",(0,o.jsx)(e.strong,{children:"/odom"}),": Odometry data (nav_msgs/Odometry)\n",(0,o.jsx)(e.strong,{children:"/scan"}),": 2D laser scan data (sensor_msgs/LaserScan)\n",(0,o.jsx)(e.strong,{children:"/camera/image_raw"}),": Raw camera images (sensor_msgs/Image)\n",(0,o.jsx)(e.strong,{children:"/tf"}),": Transform tree (tf2_msgs/TFMessage)"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Can't find a term?"})," Ask in ",(0,o.jsx)(e.a,{href:"https://github.com/asadaligith/AI-Humanoid-Robotics-Book/discussions",children:"GitHub Discussions"})," or submit a suggestion via ",(0,o.jsx)(e.a,{href:"https://github.com/asadaligith/AI-Humanoid-Robotics-Book/issues",children:"Issues"}),"."]})]})}function h(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,s)=>{s.d(e,{R:()=>t,x:()=>a});var r=s(6540);const o={},i=r.createContext(o);function t(n){const e=r.useContext(i);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:t(n.components),r.createElement(i.Provider,{value:e},n.children)}}}]);