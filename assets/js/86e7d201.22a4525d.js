"use strict";(self.webpackChunkai_humanoid_robotics_book=self.webpackChunkai_humanoid_robotics_book||[]).push([[618],{7580:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"modules/module-05-capstone/chapter-06-jetson-deployment","title":"Chapter 6: Jetson Hardware Deployment","description":"Learning Objectives","source":"@site/docs/modules/module-05-capstone/chapter-06-jetson-deployment.md","sourceDirName":"modules/module-05-capstone","slug":"/modules/module-05-capstone/chapter-06-jetson-deployment","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/chapter-06-jetson-deployment","draft":false,"unlisted":false,"editUrl":"https://github.com/asadaligith/AI-Humanoid-Robotics-Book/tree/main/docs/modules/module-05-capstone/chapter-06-jetson-deployment.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"5\ufe0f\u20e3 Simulation & Deployment","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/chapter-05-simulation-deployment"},"next":{"title":"Testing Methodology","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/testing-methodology"}}');var i=r(4848),t=r(8453);const o={sidebar_position:6},l="Chapter 6: Jetson Hardware Deployment",d={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Why Jetson for Humanoid Robotics?",id:"why-jetson-for-humanoid-robotics",level:2},{value:"Docker Containerization for ARM64",id:"docker-containerization-for-arm64",level:2},{value:"Jetson Dockerfile",id:"jetson-dockerfile",level:3},{value:"Building for Jetson",id:"building-for-jetson",level:3},{value:"Jetson Setup",id:"jetson-setup",level:2},{value:"1. Flash JetPack",id:"1-flash-jetpack",level:3},{value:"2. Install Docker",id:"2-install-docker",level:3},{value:"3. Install NVIDIA Container Runtime",id:"3-install-nvidia-container-runtime",level:3},{value:"4. Deploy Container",id:"4-deploy-container",level:3},{value:"Resource Monitoring",id:"resource-monitoring",level:2},{value:"Real-Time Monitoring Script",id:"real-time-monitoring-script",level:3},{value:"Optimization Strategies",id:"optimization-strategies",level:2},{value:"1. Model Quantization (Whisper)",id:"1-model-quantization-whisper",level:3},{value:"2. TensorRT Acceleration (Object Detection)",id:"2-tensorrt-acceleration-object-detection",level:3},{value:"3. Reduce Model Size",id:"3-reduce-model-size",level:3},{value:"Performance Benchmarks",id:"performance-benchmarks",level:2},{value:"Expected Metrics (Jetson Orin Nano 8GB)",id:"expected-metrics-jetson-orin-nano-8gb",level:3},{value:"Deployment Checklist",id:"deployment-checklist",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Research &amp; Evidence",id:"research--evidence",level:2},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-6-jetson-hardware-deployment",children:"Chapter 6: Jetson Hardware Deployment"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,i.jsx)(n.strong,{children:"Package"})," ROS 2 applications as Docker containers for ARM64 architecture"]}),"\n",(0,i.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,i.jsx)(n.strong,{children:"Deploy"})," autonomous systems to NVIDIA Jetson Orin embedded platforms"]}),"\n",(0,i.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,i.jsx)(n.strong,{children:"Monitor"})," CPU, GPU, and memory utilization on resource-constrained hardware"]}),"\n",(0,i.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,i.jsx)(n.strong,{children:"Optimize"})," AI models for edge inference (INT8 quantization, TensorRT)"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Completed Chapters 1-5 (working Gazebo simulation)"}),"\n",(0,i.jsx)(n.li,{children:"NVIDIA Jetson Orin Nano or NX (4GB-16GB RAM)"}),"\n",(0,i.jsx)(n.li,{children:"Understanding of Docker containerization"}),"\n",(0,i.jsx)(n.li,{children:"Familiarity with Linux system administration"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"Simulation validates your algorithms, but real-world deployment requires adapting to hardware constraints. The NVIDIA Jetson Orin platform provides embedded GPU acceleration ideal for AI robotics, but demands careful resource management and optimization. This chapter guides you through containerizing the capstone system, deploying to Jetson hardware, and achieving real-time performance on embedded AI accelerators."}),"\n",(0,i.jsx)(n.h2,{id:"why-jetson-for-humanoid-robotics",children:"Why Jetson for Humanoid Robotics?"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"NVIDIA Jetson Orin"})," advantages:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Embedded GPU"}),": 1024-2048 CUDA cores for AI inference"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Power Efficiency"}),": 7-25W TDP (vs. 300W+ desktop GPUs)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Form Factor"}),": Credit card size, mountable on robot chassis"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Software Stack"}),": Full ROS 2 support, CUDA, TensorRT acceleration"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Jetson Orin Models"})," (as of 2024):"]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"RAM"}),(0,i.jsx)(n.th,{children:"CUDA Cores"}),(0,i.jsx)(n.th,{children:"AI Performance"}),(0,i.jsx)(n.th,{children:"Power"}),(0,i.jsx)(n.th,{children:"Price"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Orin Nano"}),(0,i.jsx)(n.td,{children:"4-8GB"}),(0,i.jsx)(n.td,{children:"1024"}),(0,i.jsx)(n.td,{children:"40 TOPS"}),(0,i.jsx)(n.td,{children:"7-15W"}),(0,i.jsx)(n.td,{children:"$499"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Orin NX"}),(0,i.jsx)(n.td,{children:"8-16GB"}),(0,i.jsx)(n.td,{children:"1024"}),(0,i.jsx)(n.td,{children:"70 TOPS"}),(0,i.jsx)(n.td,{children:"10-25W"}),(0,i.jsx)(n.td,{children:"$699"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"AGX Orin"}),(0,i.jsx)(n.td,{children:"32-64GB"}),(0,i.jsx)(n.td,{children:"2048"}),(0,i.jsx)(n.td,{children:"275 TOPS"}),(0,i.jsx)(n.td,{children:"15-60W"}),(0,i.jsx)(n.td,{children:"$1,999"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Recommended"}),": Orin Nano 8GB for capstone demo (balances cost and performance)"]}),"\n",(0,i.jsx)(n.h2,{id:"docker-containerization-for-arm64",children:"Docker Containerization for ARM64"}),"\n",(0,i.jsx)(n.h3,{id:"jetson-dockerfile",children:"Jetson Dockerfile"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"File"}),": ",(0,i.jsx)(n.code,{children:"examples/module-05-capstone/docker/Dockerfile.jetson"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-dockerfile",children:'# Base image: NVIDIA Jetson container with ROS 2 Humble\r\nFROM dustynv/ros:humble-pytorch-l4t-r35.3.1\r\n\r\n# Set environment variables\r\nENV DEBIAN_FRONTEND=noninteractive\r\nENV ROS_DISTRO=humble\r\n\r\n# Install system dependencies\r\nRUN apt-get update && apt-get install -y \\\r\n    python3-pip \\\r\n    python3-dev \\\r\n    ffmpeg \\\r\n    portaudio19-dev \\\r\n    libsndfile1 \\\r\n    && rm -rf /var/lib/apt/lists/*\r\n\r\n# Install Python dependencies (optimized for Jetson)\r\nCOPY requirements.txt /tmp/\r\nRUN pip3 install --no-cache-dir \\\r\n    --extra-index-url https://download.pytorch.org/whl/cpu \\\r\n    -r /tmp/requirements.txt\r\n\r\n# Install Whisper (use base model for Jetson)\r\nRUN pip3 install openai-whisper\r\n\r\n# Copy ROS 2 workspace\r\nWORKDIR /ros2_ws\r\nCOPY . /ros2_ws/src/capstone_demo/\r\n\r\n# Build ROS 2 workspace\r\nRUN source /opt/ros/${ROS_DISTRO}/setup.bash && \\\r\n    colcon build --cmake-args -DCMAKE_BUILD_TYPE=Release\r\n\r\n# Source workspace in bashrc\r\nRUN echo "source /ros2_ws/install/setup.bash" >> /root/.bashrc\r\n\r\n# Expose ROS 2 ports\r\nEXPOSE 7400-7500\r\n\r\n# Entrypoint\r\nCMD ["bash"]\n'})}),"\n",(0,i.jsx)(n.h3,{id:"building-for-jetson",children:"Building for Jetson"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# On x86 development machine (cross-compile for ARM64)\r\ndocker buildx build --platform linux/arm64 \\\r\n  -t capstone-jetson:latest \\\r\n  -f docker/Dockerfile.jetson .\r\n\r\n# Or build directly on Jetson (slower but simpler)\r\nssh jetson@jetson-orin.local\r\ncd /path/to/AI-Humanoid-Robotics-Book\r\ndocker build -t capstone-jetson:latest -f docker/Dockerfile.jetson .\n"})}),"\n",(0,i.jsx)(n.h2,{id:"jetson-setup",children:"Jetson Setup"}),"\n",(0,i.jsx)(n.h3,{id:"1-flash-jetpack",children:"1. Flash JetPack"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# On host machine with NVIDIA SDK Manager\r\n# Download from: https://developer.nvidia.com/sdk-manager\r\n\r\n# Flash JetPack 5.1+ (includes Ubuntu 20.04, ROS 2 compatible)\r\n# Follow NVIDIA flashing guide for your Jetson model\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-install-docker",children:"2. Install Docker"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# On Jetson (SSH or direct)\r\nsudo apt-get update\r\nsudo apt-get install -y docker.io\r\nsudo usermod -aG docker $USER  # Add user to docker group\r\nnewgrp docker  # Refresh group membership\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-install-nvidia-container-runtime",children:"3. Install NVIDIA Container Runtime"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Enable GPU access in Docker containers\r\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\r\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\r\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\\r\n  sudo tee /etc/apt/sources.list.d/nvidia-docker.list\r\n\r\nsudo apt-get update\r\nsudo apt-get install -y nvidia-container-runtime\r\nsudo systemctl restart docker\n"})}),"\n",(0,i.jsx)(n.h3,{id:"4-deploy-container",children:"4. Deploy Container"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Run capstone container with GPU support\r\ndocker run -it --rm \\\r\n  --runtime=nvidia \\\r\n  --network=host \\\r\n  --device=/dev/snd:/dev/snd \\  # Microphone access\r\n  --device=/dev/video0:/dev/video0 \\  # Camera access\r\n  -e OPENAI_API_KEY=$OPENAI_API_KEY \\\r\n  -v /path/to/models:/models \\  # Mount model weights\r\n  capstone-jetson:latest\n"})}),"\n",(0,i.jsx)(n.h2,{id:"resource-monitoring",children:"Resource Monitoring"}),"\n",(0,i.jsx)(n.h3,{id:"real-time-monitoring-script",children:"Real-Time Monitoring Script"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"File"}),": ",(0,i.jsx)(n.code,{children:"scripts/deployment/monitor_resources.py"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n"""Resource monitoring for Jetson deployment"""\r\n\r\nimport time\r\nimport subprocess\r\nimport re\r\n\r\ndef get_gpu_stats():\r\n    """Query Jetson GPU utilization via tegrastats"""\r\n    try:\r\n        result = subprocess.run([\'tegrastats\', \'--interval\', \'1000\'],\r\n                                capture_output=True, text=True, timeout=2)\r\n        output = result.stdout\r\n\r\n        # Parse GPU usage (example: GR3D_FREQ 99%)\r\n        gpu_match = re.search(r\'GR3D_FREQ (\\d+)%\', output)\r\n        gpu_util = int(gpu_match.group(1)) if gpu_match else 0\r\n\r\n        return gpu_util\r\n    except Exception as e:\r\n        print(f"GPU stats error: {e}")\r\n        return 0\r\n\r\ndef get_memory_stats():\r\n    """Get RAM usage"""\r\n    with open(\'/proc/meminfo\', \'r\') as f:\r\n        lines = f.readlines()\r\n    mem_total = int([l for l in lines if \'MemTotal\' in l][0].split()[1])\r\n    mem_avail = int([l for l in lines if \'MemAvailable\' in l][0].split()[1])\r\n    mem_used_pct = ((mem_total - mem_avail) / mem_total) * 100\r\n    return mem_used_pct\r\n\r\ndef get_cpu_temp():\r\n    """Get CPU temperature"""\r\n    try:\r\n        with open(\'/sys/devices/virtual/thermal/thermal_zone0/temp\', \'r\') as f:\r\n            temp = int(f.read().strip()) / 1000.0  # Convert to Celsius\r\n        return temp\r\n    except:\r\n        return 0.0\r\n\r\ndef monitor_loop():\r\n    """Continuous monitoring loop"""\r\n    print("Timestamp,GPU%,RAM%,CPU_Temp_C")\r\n    while True:\r\n        gpu = get_gpu_stats()\r\n        ram = get_memory_stats()\r\n        temp = get_cpu_temp()\r\n\r\n        print(f"{time.time():.1f},{gpu},{ram:.1f},{temp:.1f}")\r\n        time.sleep(1)\r\n\r\nif __name__ == \'__main__\':\r\n    monitor_loop()\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Usage"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Run monitoring in background\r\npython3 scripts/deployment/monitor_resources.py > jetson_metrics.csv &\r\n\r\n# View real-time stats\r\ntail -f jetson_metrics.csv\n"})}),"\n",(0,i.jsx)(n.h2,{id:"optimization-strategies",children:"Optimization Strategies"}),"\n",(0,i.jsx)(n.h3,{id:"1-model-quantization-whisper",children:"1. Model Quantization (Whisper)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Convert Whisper to INT8 for faster inference\r\nimport whisper\r\nfrom torch.quantization import quantize_dynamic\r\n\r\nmodel = whisper.load_model("base")\r\nquantized_model = quantize_dynamic(\r\n    model, {torch.nn.Linear}, dtype=torch.qint8\r\n)\r\n\r\n# Save quantized model\r\ntorch.save(quantized_model.state_dict(), "whisper_base_int8.pth")\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Speedup"}),": 2-3x faster inference on Jetson with minimal accuracy loss"]}),"\n",(0,i.jsx)(n.h3,{id:"2-tensorrt-acceleration-object-detection",children:"2. TensorRT Acceleration (Object Detection)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Convert YOLO to TensorRT engine (once, offline)\r\ntrtexec --onnx=yolov8n.onnx \\\r\n  --saveEngine=yolov8n_fp16.trt \\\r\n  --fp16  # Use FP16 precision for Jetson\r\n\r\n# Use TensorRT engine in detection node (faster inference)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-reduce-model-size",children:"3. Reduce Model Size"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Whisper"}),": Use ",(0,i.jsx)(n.code,{children:"tiny"})," or ",(0,i.jsx)(n.code,{children:"base"})," instead of ",(0,i.jsx)(n.code,{children:"medium"})," (39M vs. 769M params)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"YOLO"}),": Use YOLOv8n (nano) instead of YOLOv8l (large)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM"}),": Use local Llama 2 7B instead of GPT-4 API calls"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,i.jsx)(n.h3,{id:"expected-metrics-jetson-orin-nano-8gb",children:"Expected Metrics (Jetson Orin Nano 8GB)"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Component"}),(0,i.jsx)(n.th,{children:"Latency"}),(0,i.jsx)(n.th,{children:"GPU Usage"}),(0,i.jsx)(n.th,{children:"RAM Usage"}),(0,i.jsx)(n.th,{children:"Notes"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Voice Transcription (Whisper base)"}),(0,i.jsx)(n.td,{children:"2-4s"}),(0,i.jsx)(n.td,{children:"60-80%"}),(0,i.jsx)(n.td,{children:"500MB"}),(0,i.jsx)(n.td,{children:"Per 3s audio clip"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"LLM Planning (API)"}),(0,i.jsx)(n.td,{children:"5-10s"}),(0,i.jsx)(n.td,{children:"0%"}),(0,i.jsx)(n.td,{children:"50MB"}),(0,i.jsx)(n.td,{children:"Network dependent"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Navigation (Nav2)"}),(0,i.jsx)(n.td,{children:"Real-time"}),(0,i.jsx)(n.td,{children:"10-20%"}),(0,i.jsx)(n.td,{children:"200MB"}),(0,i.jsx)(n.td,{children:"Path planning"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Object Detection (YOLO)"}),(0,i.jsx)(n.td,{children:"30-50ms"}),(0,i.jsx)(n.td,{children:"70-90%"}),(0,i.jsx)(n.td,{children:"300MB"}),(0,i.jsx)(n.td,{children:"At 20 FPS"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Manipulation (MoveIt 2)"}),(0,i.jsx)(n.td,{children:"Real-time"}),(0,i.jsx)(n.td,{children:"5-10%"}),(0,i.jsx)(n.td,{children:"150MB"}),(0,i.jsx)(n.td,{children:"IK solver"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Total End-to-End"}),": 35-50 seconds (vs. 30-40s on desktop GPU)"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Resource Budget"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RAM"}),": 2-3GB peak (safe margin on 8GB Jetson)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GPU"}),": 80-90% during Whisper + YOLO concurrent"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Power"}),": 12-18W sustained (within 7-15W envelope if optimized)"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"deployment-checklist",children:"Deployment Checklist"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Pre-Deployment"}),":"]}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","JetPack 5.1+ flashed with ROS 2 support"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Docker and NVIDIA runtime installed"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Container built for ARM64 architecture"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Model weights pre-loaded (avoid downloading on Jetson)"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","OpenAI API key configured (or local LLM ready)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Deployment"}),":"]}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Container runs successfully with GPU access"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Microphone and camera devices accessible"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","ROS 2 nodes launch without errors"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Resource monitoring shows acceptable utilization"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Validation"}),":"]}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Voice command transcribed correctly"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Object detection achieves \u226585% accuracy"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","End-to-end task completes in <60 seconds"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","System stable for 30+ minutes continuous operation"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Issue"}),": Out of memory errors\r\n",(0,i.jsx)(n.strong,{children:"Solution"}),": Reduce batch sizes, use INT8 models, close unused processes"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Issue"}),": GPU not accessible in container\r\n",(0,i.jsx)(n.strong,{children:"Solution"}),": Verify ",(0,i.jsx)(n.code,{children:"--runtime=nvidia"})," flag, check nvidia-container-runtime installation"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Issue"}),": Whisper too slow (>10s per transcription)\r\n",(0,i.jsx)(n.strong,{children:"Solution"}),": Use ",(0,i.jsx)(n.code,{children:"tiny"})," model or quantize to INT8"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Issue"}),": Thermal throttling (CPU temp >80\xb0C)\r\n",(0,i.jsx)(n.strong,{children:"Solution"}),": Add heatsink/fan, reduce concurrent GPU workloads, lower power mode"]}),"\n",(0,i.jsx)(n.h2,{id:"research--evidence",children:"Research & Evidence"}),"\n",(0,i.jsx)(n.p,{children:"Jetson deployment strategies informed by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["NVIDIA Jetson Orin Documentation: ",(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/embedded/jetson-orin",children:"developer.nvidia.com/embedded/jetson-orin"})]}),"\n",(0,i.jsxs)(n.li,{children:["TensorRT Optimization Guide: ",(0,i.jsx)(n.a,{href:"https://docs.nvidia.com/deeplearning/tensorrt/",children:"docs.nvidia.com/deeplearning/tensorrt"})]}),"\n",(0,i.jsxs)(n.li,{children:["ROS 2 on Jetson: ",(0,i.jsx)(n.a,{href:"https://nvidia-ai-iot.github.io/ros2_jetson/",children:"nvidia-ai-iot.github.io/ros2_jetson"})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"Deploying autonomous systems to embedded hardware requires balancing performance, power, and cost:"}),"\n",(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Docker containers"})," enable reproducible deployment across development and production environments"]}),"\n",(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Model optimization"})," (quantization, TensorRT) achieves real-time performance on Jetson's 15W power budget"]}),"\n",(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Resource monitoring"})," prevents out-of-memory crashes and thermal issues during extended operation"]}),"\n",(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"Jetson Orin"})," provides desktop-class AI inference in an embeddable form factor ideal for humanoid robots"]}),"\n",(0,i.jsx)(n.p,{children:"Your capstone system now runs on hardware that can be mounted on a physical robot\u2014bringing autonomous intelligence from simulation to reality."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next Steps"}),": Return to ",(0,i.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/chapter-01-architecture",children:"Chapter 1: System Architecture"})," to review the complete integrated system, or explore ",(0,i.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/benchmarking",children:"Performance Benchmarking"})," to quantify your deployment's efficiency."]}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.p,{children:["\u2b50 ",(0,i.jsx)(n.strong,{children:"Exercise 1"}),": Measure Whisper latency on Jetson with ",(0,i.jsx)(n.code,{children:"base"}),", ",(0,i.jsx)(n.code,{children:"small"}),", and ",(0,i.jsx)(n.code,{children:"tiny"})," models. Plot latency vs. accuracy tradeoff."]}),"\n",(0,i.jsxs)(n.p,{children:["\u2b50\u2b50 ",(0,i.jsx)(n.strong,{children:"Exercise 2"}),": Convert the YOLO model to TensorRT and benchmark FPS improvement. Document memory footprint changes."]}),"\n",(0,i.jsxs)(n.p,{children:["\u2b50\u2b50\u2b50 ",(0,i.jsx)(n.strong,{children:"Exercise 3"}),": Implement a ",(0,i.jsx)(n.strong,{children:"dynamic resource allocator"})," that reduces model precision (FP16\u2192INT8) if GPU temperature exceeds 75\xb0C."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Word Count"}),": 247 words (Target: 200-250) \u2705"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>l});var s=r(6540);const i={},t=s.createContext(i);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);