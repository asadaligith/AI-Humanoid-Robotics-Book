"use strict";(self.webpackChunkai_humanoid_robotics_book=self.webpackChunkai_humanoid_robotics_book||[]).push([[7421],{635:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>a});const o=JSON.parse('{"id":"modules/module-04-vla-vision-language-action/index","title":"Module 4: Vision-Language-Action (VLA)","description":"Integrate voice commands, LLM planning, and robot execution for natural human-robot interaction","source":"@site/docs/modules/module-04-vla-vision-language-action/index.md","sourceDirName":"modules/module-04-vla-vision-language-action","slug":"/modules/module-04-vla-vision-language-action/","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-04-vla-vision-language-action/","draft":false,"unlisted":false,"editUrl":"https://github.com/asadaligith/AI-Humanoid-Robotics-Book/tree/main/docs/modules/module-04-vla-vision-language-action/index.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Module 4: Vision-Language-Action (VLA)","description":"Integrate voice commands, LLM planning, and robot execution for natural human-robot interaction","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"7\ufe0f\u20e3 Module Summary","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-03-ai-robot-brain-isaac/chapter-07-summary"},"next":{"title":"1\ufe0f\u20e3 Introduction to VLA","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-04-vla-vision-language-action/chapter-01-introduction"}}');var t=i(4848),s=i(8453);const r={title:"Module 4: Vision-Language-Action (VLA)",description:"Integrate voice commands, LLM planning, and robot execution for natural human-robot interaction",sidebar_position:4},l="Module 4: Vision-Language-Action (VLA)",d={},a=[{value:"Module Overview",id:"module-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"Real-World Application",id:"real-world-application",level:2},{value:"Resources",id:"resources",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.h2,{id:"module-overview",children:"Module Overview"}),"\n",(0,t.jsx)(n.p,{children:"This module teaches Vision-Language-Action (VLA) pipelines, enabling robots to understand natural language commands and execute complex tasks autonomously."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement"})," speech-to-text using Whisper"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integrate"})," LLMs (Claude/GPT-4) for task planning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Execute"})," action plans via ROS 2"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deploy"})," VLA pipeline on Jetson for real-time operation"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Modules 1-3"}),": ROS 2, simulation, perception, navigation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"API Keys"}),": OpenAI API (Whisper, GPT-4) OR Anthropic Claude API"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Chapter"}),(0,t.jsx)(n.th,{children:"Topic"}),(0,t.jsx)(n.th,{children:"Key Technology"}),(0,t.jsx)(n.th,{children:"Time"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-04-vla-vision-language-action/chapter-01-introduction",children:"Chapter 1"})}),(0,t.jsx)(n.td,{children:"VLA Introduction"}),(0,t.jsx)(n.td,{children:"LLMs in robotics"}),(0,t.jsx)(n.td,{children:"30 min"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-04-vla-vision-language-action/chapter-02-whisper",children:"Chapter 2"})}),(0,t.jsx)(n.td,{children:"Speech Recognition"}),(0,t.jsx)(n.td,{children:"OpenAI Whisper"}),(0,t.jsx)(n.td,{children:"1.5 hours"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-04-vla-vision-language-action/chapter-03-llm-planning",children:"Chapter 3"})}),(0,t.jsx)(n.td,{children:"LLM Task Planning"}),(0,t.jsx)(n.td,{children:"Claude/GPT-4"}),(0,t.jsx)(n.td,{children:"2 hours"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-04-vla-vision-language-action/chapter-04-ros-executor",children:"Chapter 4"})}),(0,t.jsx)(n.td,{children:"ROS Action Executor"}),(0,t.jsx)(n.td,{children:"ROS 2 actions"}),(0,t.jsx)(n.td,{children:"1.5 hours"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-04-vla-vision-language-action/chapter-05-integration",children:"Chapter 5"})}),(0,t.jsx)(n.td,{children:"End-to-End Pipeline"}),(0,t.jsx)(n.td,{children:"Full VLA"}),(0,t.jsx)(n.td,{children:"2 hours"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-04-vla-vision-language-action/chapter-06-jetson-deploy",children:"Chapter 6"})}),(0,t.jsx)(n.td,{children:"Jetson Deployment"}),(0,t.jsx)(n.td,{children:"Edge AI"}),(0,t.jsx)(n.td,{children:"2 hours"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-04-vla-vision-language-action/chapter-07-summary",children:"Chapter 7"})}),(0,t.jsx)(n.td,{children:"Module Summary"}),(0,t.jsx)(n.td,{children:"N/A"}),(0,t.jsx)(n.td,{children:"30 min"})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Total Time"}),": 10-12 hours"]}),"\n",(0,t.jsx)(n.h2,{id:"real-world-application",children:"Real-World Application"}),"\n",(0,t.jsx)(n.p,{children:"VLA enables:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"Pick up the red cube" \u2192 Robot navigates, grasps, delivers'}),"\n",(0,t.jsx)(n.li,{children:'"Follow me" \u2192 Robot tracks and follows human'}),"\n",(0,t.jsx)(n.li,{children:'"Clean the table" \u2192 Robot identifies objects, grasps, places in bin'}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/speech-to-text",children:"Whisper Documentation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://www.anthropic.com/api",children:"Anthropic Claude API"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2310.07642",children:"RoboInstructor Paper"})}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-04-vla-vision-language-action/chapter-01-introduction",children:"Start Chapter 1"})})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var o=i(6540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);