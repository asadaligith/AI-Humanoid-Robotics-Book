"use strict";(self.webpackChunkai_humanoid_robotics_book=self.webpackChunkai_humanoid_robotics_book||[]).push([[2791],{6551:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"modules/module-05-capstone/chapter-01-architecture","title":"Chapter 1: System Architecture","description":"Learning Objectives","source":"@site/docs/modules/module-05-capstone/chapter-01-architecture.md","sourceDirName":"modules/module-05-capstone","slug":"/modules/module-05-capstone/chapter-01-architecture","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/chapter-01-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/asadaligith/AI-Humanoid-Robotics-Book/tree/main/docs/modules/module-05-capstone/chapter-01-architecture.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 5: Capstone - The Autonomous Humanoid","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/"},"next":{"title":"2\ufe0f\u20e3 Voice & LLM Integration","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/chapter-02-voice-llm"}}');var s=i(4848),r=i(8453);const o={sidebar_position:1},a="Chapter 1: System Architecture",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"System Overview",id:"system-overview",level:2},{value:"Five Core Capabilities",id:"five-core-capabilities",level:3},{value:"Integration Architecture",id:"integration-architecture",level:3},{value:"State Machine Design",id:"state-machine-design",level:2},{value:"State Definitions",id:"state-definitions",level:3},{value:"Transition Logic",id:"transition-logic",level:3},{value:"Data Flow Patterns",id:"data-flow-patterns",level:2},{value:"Nominal Execution Flow",id:"nominal-execution-flow",level:3},{value:"Failure Scenarios",id:"failure-scenarios",level:3},{value:"ROS 2 Node Architecture",id:"ros-2-node-architecture",level:2},{value:"Node Topology",id:"node-topology",level:3},{value:"Communication Patterns",id:"communication-patterns",level:3},{value:"Integration Challenges and Solutions",id:"integration-challenges-and-solutions",level:2},{value:"Challenge 1: Asynchronous Timing",id:"challenge-1-asynchronous-timing",level:3},{value:"Challenge 2: Coordinate Frame Transforms",id:"challenge-2-coordinate-frame-transforms",level:3},{value:"Challenge 3: Failure Recovery Without Infinite Loops",id:"challenge-3-failure-recovery-without-infinite-loops",level:3},{value:"Research &amp; Evidence",id:"research--evidence",level:2},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-1-system-architecture",children:"Chapter 1: System Architecture"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,s.jsx)(n.strong,{children:"Design"})," integrated autonomous systems combining perception, planning, navigation, and manipulation"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,s.jsx)(n.strong,{children:"Analyze"})," system architectures using state machines and data flow diagrams"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,s.jsx)(n.strong,{children:"Identify"})," failure modes and design recovery strategies for multi-component systems"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,s.jsx)(n.strong,{children:"Apply"})," ROS 2 communication patterns (topics, services, actions) for distributed robot control"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Completed Modules 1-4 (ROS 2, Gazebo, Isaac, VLA)"}),"\n",(0,s.jsx)(n.li,{children:"Understanding of state machines and finite state automata"}),"\n",(0,s.jsx)(n.li,{children:"Familiarity with system design diagrams (block diagrams, sequence diagrams)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"Autonomous humanoid robots require orchestrating multiple AI capabilities\u2014voice recognition, language understanding, navigation, perception, and manipulation\u2014into a cohesive intelligence. The challenge isn't just making each component work independently, but ensuring they communicate reliably, handle failures gracefully, and coordinate timing across asynchronous operations."}),"\n",(0,s.jsx)(n.p,{children:"This chapter presents the system architecture for a voice-commanded fetch-and-deliver robot that integrates five core capabilities. You'll learn how to structure complex autonomous systems using state machines, how to design ROS 2 node topologies for distributed control, and how to anticipate and handle failure modes that inevitably arise in real-world deployment."}),"\n",(0,s.jsx)(n.h2,{id:"system-overview",children:"System Overview"}),"\n",(0,s.jsxs)(n.p,{children:["The autonomous humanoid system follows a ",(0,s.jsx)(n.strong,{children:"perception-planning-action"})," loop, extended with natural language grounding:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Voice Command \u2192 Transcription \u2192 Task Planning \u2192 Navigation \u2192 Perception \u2192 Manipulation \u2192 Delivery\n"})}),"\n",(0,s.jsx)(n.p,{children:"Each arrow represents a state transition in the execution pipeline. The system moves sequentially through states, with branching logic for failures and clarifications."}),"\n",(0,s.jsx)(n.h3,{id:"five-core-capabilities",children:"Five Core Capabilities"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"1. Voice Input (Whisper)"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Captures audio from microphone"}),"\n",(0,s.jsx)(n.li,{children:"Transcribes to text using OpenAI Whisper"}),"\n",(0,s.jsxs)(n.li,{children:["Publishes transcribed commands to ",(0,s.jsx)(n.code,{children:"/voice/transcribed_text"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"2. LLM Planning (GPT-4/Claude)"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Receives voice commands and capability manifest"}),"\n",(0,s.jsx)(n.li,{children:"Generates structured action sequences (JSON)"}),"\n",(0,s.jsx)(n.li,{children:"Handles ambiguity with clarification questions"}),"\n",(0,s.jsxs)(n.li,{children:["Publishes plans to ",(0,s.jsx)(n.code,{children:"/planning/action_sequence"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"3. Navigation (VSLAM + Nav2)"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Localizes robot in environment using visual odometry"}),"\n",(0,s.jsx)(n.li,{children:"Plans collision-free paths using costmaps"}),"\n",(0,s.jsx)(n.li,{children:"Executes navigation goals via ROS 2 actions"}),"\n",(0,s.jsxs)(n.li,{children:["Reports status via ",(0,s.jsx)(n.code,{children:"/navigation/status"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"4. Object Detection (Computer Vision)"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Processes camera images for object detection"}),"\n",(0,s.jsx)(n.li,{children:"Identifies target objects by class and attributes (color, shape)"}),"\n",(0,s.jsxs)(n.li,{children:["Publishes detections to ",(0,s.jsx)(n.code,{children:"/perception/detected_objects"})]}),"\n",(0,s.jsx)(n.li,{children:"Computes 3D coordinates using depth information"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"5. Manipulation (MoveIt 2)"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Receives pick/place goals from integration controller"}),"\n",(0,s.jsx)(n.li,{children:"Computes inverse kinematics and motion plans"}),"\n",(0,s.jsx)(n.li,{children:"Executes grasps with force feedback"}),"\n",(0,s.jsx)(n.li,{children:"Retries failed grasps up to 3 times"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"integration-architecture",children:"Integration Architecture"}),"\n",(0,s.jsxs)(n.p,{children:["The system uses a ",(0,s.jsx)(n.strong,{children:"centralized state machine"})," (integration_demo.py) that coordinates all five capabilities:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Integration Demo (State Machine)                \u2502\n\u2502                                                               \u2502\n\u2502  States: IDLE \u2192 LISTENING \u2192 TRANSCRIBING \u2192 PLANNING \u2192        \u2502\n\u2502          VALIDATING \u2192 NAVIGATING \u2192 PERCEIVING \u2192              \u2502\n\u2502          MANIPULATING \u2192 COMPLETED/FAILED                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502           \u2502            \u2502            \u2502          \u2502\n         \u25bc           \u25bc            \u25bc            \u25bc          \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Voice   \u2502 \u2502   LLM    \u2502 \u2502   Nav2   \u2502 \u2502 Object \u2502 \u2502 MoveIt \u2502\n   \u2502  Input  \u2502 \u2502 Planner  \u2502 \u2502Controller\u2502 \u2502Detector\u2502 \u2502Control \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502           \u2502            \u2502            \u2502          \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         ROS 2 Topics/Actions\n"})}),"\n",(0,s.jsxs)(n.p,{children:["See ",(0,s.jsx)(n.a,{href:"../../../assets/diagrams/architecture/capstone-architecture.png",children:"Architecture Diagram"})," for detailed node topology and ",(0,s.jsx)(n.a,{href:"../../../specs/005-capstone-autonomous-humanoid/contracts/ros2-interfaces.md",children:"ROS 2 Interfaces Contract"})," for complete interface specifications."]}),"\n",(0,s.jsx)(n.h2,{id:"state-machine-design",children:"State Machine Design"}),"\n",(0,s.jsxs)(n.p,{children:["The system uses an ",(0,s.jsx)(n.strong,{children:"11-state finite state machine"})," to track execution progress and handle failures:"]}),"\n",(0,s.jsx)(n.h3,{id:"state-definitions",children:"State Definitions"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"State"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Entry Condition"}),(0,s.jsx)(n.th,{children:"Exit Condition"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"IDLE"})}),(0,s.jsx)(n.td,{children:"Waiting for voice command"}),(0,s.jsx)(n.td,{children:"System startup or task completion"}),(0,s.jsx)(n.td,{children:"Voice activity detected"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"LISTENING"})}),(0,s.jsx)(n.td,{children:"Capturing audio"}),(0,s.jsx)(n.td,{children:"Voice activity detected"}),(0,s.jsx)(n.td,{children:"Audio capture complete (3s)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"TRANSCRIBING"})}),(0,s.jsx)(n.td,{children:"Whisper processing"}),(0,s.jsx)(n.td,{children:"Audio available"}),(0,s.jsx)(n.td,{children:"Text transcription ready"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"PLANNING"})}),(0,s.jsx)(n.td,{children:"LLM generating action plan"}),(0,s.jsx)(n.td,{children:"Transcribed text available"}),(0,s.jsx)(n.td,{children:"Plan generated or timeout (10s)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"VALIDATING"})}),(0,s.jsx)(n.td,{children:"Checking plan feasibility"}),(0,s.jsx)(n.td,{children:"Plan received"}),(0,s.jsx)(n.td,{children:"Plan valid or invalid"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"NAVIGATING"})}),(0,s.jsx)(n.td,{children:"Moving to target location"}),(0,s.jsx)(n.td,{children:"Valid navigation goal"}),(0,s.jsx)(n.td,{children:"Arrived or navigation failed"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"PERCEIVING"})}),(0,s.jsx)(n.td,{children:"Detecting target object"}),(0,s.jsx)(n.td,{children:"Arrived at location"}),(0,s.jsx)(n.td,{children:"Object detected or not found"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"MANIPULATING"})}),(0,s.jsx)(n.td,{children:"Picking/placing object"}),(0,s.jsx)(n.td,{children:"Object localized"}),(0,s.jsx)(n.td,{children:"Grasp succeeded or failed (3 retries)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"COMPLETED"})}),(0,s.jsx)(n.td,{children:"Task finished successfully"}),(0,s.jsx)(n.td,{children:"Object delivered"}),(0,s.jsx)(n.td,{children:"Transition to IDLE"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"FAILED"})}),(0,s.jsx)(n.td,{children:"Error occurred"}),(0,s.jsx)(n.td,{children:"Unrecoverable failure in any state"}),(0,s.jsx)(n.td,{children:"Log error, transition to IDLE"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"AWAITING_CLARIFICATION"})}),(0,s.jsx)(n.td,{children:"LLM needs more info"}),(0,s.jsx)(n.td,{children:"Ambiguous command detected"}),(0,s.jsx)(n.td,{children:"User provides clarification"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"transition-logic",children:"Transition Logic"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Pseudocode for state machine transitions\ndef state_transition(current_state, event):\n    if current_state == State.IDLE and event == Event.VOICE_DETECTED:\n        return State.LISTENING\n\n    elif current_state == State.LISTENING and event == Event.AUDIO_CAPTURED:\n        return State.TRANSCRIBING\n\n    elif current_state == State.TRANSCRIBING and event == Event.TEXT_READY:\n        return State.PLANNING\n\n    elif current_state == State.PLANNING:\n        if event == Event.PLAN_GENERATED:\n            return State.VALIDATING\n        elif event == Event.AMBIGUOUS_COMMAND:\n            return State.AWAITING_CLARIFICATION\n        elif event == Event.TIMEOUT:\n            return State.FAILED\n\n    elif current_state == State.NAVIGATING:\n        if event == Event.NAVIGATION_SUCCEEDED:\n            return State.PERCEIVING\n        elif event == Event.NAVIGATION_FAILED:\n            return State.FAILED  # Could add replanning logic here\n\n    # ... (complete transition table in code example)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Full state machine implementation: ",(0,s.jsx)(n.code,{children:"examples/module-05-capstone/integration_demo.py:StateMachine"})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Detailed Specification"}),": See ",(0,s.jsx)(n.a,{href:"../../../specs/005-capstone-autonomous-humanoid/contracts/state-machine.md",children:"State Machine Contract"})," for complete state definitions, transition logic, retry budgets, and failure recovery strategies."]}),"\n",(0,s.jsx)(n.h2,{id:"data-flow-patterns",children:"Data Flow Patterns"}),"\n",(0,s.jsx)(n.h3,{id:"nominal-execution-flow",children:"Nominal Execution Flow"}),"\n",(0,s.jsxs)(n.p,{children:["For the command ",(0,s.jsx)(n.em,{children:'"Bring me the red cup from the kitchen table"'}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice \u2192 LLM"}),": ",(0,s.jsx)(n.code,{children:"/voice/transcribed_text"})," (std_msgs/String)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM \u2192 Integration"}),": ",(0,s.jsx)(n.code,{children:"/planning/action_sequence"})," (custom_msgs/ActionPlan)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integration \u2192 Nav2"}),": ",(0,s.jsx)(n.code,{children:"/navigation/navigate_to_pose"})," (nav2_msgs/NavigateToPose action)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integration \u2192 Perception"}),": Trigger object detection on ",(0,s.jsx)(n.code,{children:"/perception/detected_objects"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integration \u2192 Manipulation"}),": ",(0,s.jsx)(n.code,{children:"/manipulation/pick_object"})," (custom_msgs/PickObject action)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Navigation (return) \u2192 Manipulation (place)"}),": Repeat steps 3-5 for delivery"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["See ",(0,s.jsx)(n.a,{href:"../../../assets/diagrams/architecture/vla-pipeline.png",children:"Data Flow Diagram"})," for visual representation. For detailed message formats and communication patterns, refer to ",(0,s.jsx)(n.a,{href:"../../../specs/005-capstone-autonomous-humanoid/contracts/ros2-interfaces.md",children:"ROS 2 Interfaces Contract"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"failure-scenarios",children:"Failure Scenarios"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Scenario 1: Navigation Blocked"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Symptom"}),": Nav2 action returns ",(0,s.jsx)(n.code,{children:"ABORTED"})," status"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Recovery"}),": Replan with updated costmap, retry navigation (max 3 attempts)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fallback"}),": Request user to clear obstacle or abort task"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Scenario 2: Object Not Found"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Symptom"}),": Object detector returns empty detections after 10 seconds"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Recovery"}),": Ask LLM to clarify object description, retry detection from different viewpoint"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fallback"}),": Request user to point out object or abort task"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["See ",(0,s.jsx)(n.a,{href:"../../../specs/005-capstone-autonomous-humanoid/contracts/state-machine.md",children:"State Machine Contract"})," for complete failure handling logic and ",(0,s.jsx)(n.a,{href:"../../../assets/diagrams/architecture/nominal-flow.png",children:"nominal"})," vs ",(0,s.jsx)(n.a,{href:"../../../assets/diagrams/architecture/navigation-failure.png",children:"failure scenario diagrams"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"ros-2-node-architecture",children:"ROS 2 Node Architecture"}),"\n",(0,s.jsxs)(n.p,{children:["The system uses ",(0,s.jsx)(n.strong,{children:"5 primary ROS 2 nodes"}),", each encapsulating a core capability:"]}),"\n",(0,s.jsx)(n.h3,{id:"node-topology",children:"Node Topology"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"/voice_input_node\n  Publishers: /voice/transcribed_text (std_msgs/String)\n\n/llm_planner_node\n  Subscribers: /voice/transcribed_text, /system/capability_manifest\n  Publishers: /planning/action_sequence (custom_msgs/ActionPlan)\n  Services: /planning/validate_plan (custom_srvs/ValidatePlan)\n\n/navigation_controller\n  Action Servers: /navigation/navigate_to_pose (nav2_msgs/NavigateToPose)\n  Publishers: /navigation/status (std_msgs/String)\n\n/object_detection_node\n  Subscribers: /camera/image_raw (sensor_msgs/Image)\n  Publishers: /perception/detected_objects (vision_msgs/Detection2DArray)\n\n/manipulation_controller\n  Action Servers: /manipulation/pick_object, /manipulation/place_object\n  Subscribers: /perception/detected_objects\n"})}),"\n",(0,s.jsxs)(n.p,{children:["See ",(0,s.jsx)(n.a,{href:"../../../specs/005-capstone-autonomous-humanoid/contracts/ros2-interfaces.md",children:"ROS 2 Interfaces"})," for detailed message/service/action definitions."]}),"\n",(0,s.jsx)(n.h3,{id:"communication-patterns",children:"Communication Patterns"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Topics"})," (pub/sub, loose coupling):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Voice transcriptions, object detections, status updates"}),"\n",(0,s.jsx)(n.li,{children:"High-frequency sensor data (camera images at 30 Hz)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Actions"})," (goal-oriented, feedback, cancellable):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Navigation goals (feedback: distance remaining, ETA)"}),"\n",(0,s.jsx)(n.li,{children:"Manipulation goals (feedback: grasp phase, joint positions)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Services"})," (synchronous RPC):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Plan validation (LLM checks feasibility before execution)"}),"\n",(0,s.jsx)(n.li,{children:"Pose queries (get current robot pose from localization)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"integration-challenges-and-solutions",children:"Integration Challenges and Solutions"}),"\n",(0,s.jsx)(n.h3,{id:"challenge-1-asynchronous-timing",children:"Challenge 1: Asynchronous Timing"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Voice transcription takes 2-3 seconds, LLM planning takes 5-10 seconds, navigation takes 10-30 seconds. How to coordinate without blocking?"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solution"}),": Use ROS 2 ",(0,s.jsx)(n.strong,{children:"action clients"})," with async/await patterns. Integration controller waits for action results using callbacks, allowing other operations to proceed concurrently."]}),"\n",(0,s.jsx)(n.h3,{id:"challenge-2-coordinate-frame-transforms",children:"Challenge 2: Coordinate Frame Transforms"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Object detections are in camera frame, navigation goals are in world frame, manipulation is in base_link frame."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solution"}),": Use ",(0,s.jsx)(n.strong,{children:"tf2"})," library for automatic coordinate transforms. All nodes publish their transforms, and tf2 handles conversions:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"transform = tf_buffer.lookup_transform('base_link', 'camera_optical_frame', rclpy.time.Time())\nobject_in_base = do_transform_point(object_in_camera, transform)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"challenge-3-failure-recovery-without-infinite-loops",children:"Challenge 3: Failure Recovery Without Infinite Loops"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Navigation might fail repeatedly if obstacle is permanent. Grasp might fail if object is too slippery."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solution"}),": ",(0,s.jsx)(n.strong,{children:"Retry budgets"})," - Each operation gets max 3 attempts before transitioning to FAILED state. Integration controller logs failure reason and requests human intervention."]}),"\n",(0,s.jsx)(n.h2,{id:"research--evidence",children:"Research & Evidence"}),"\n",(0,s.jsx)(n.p,{children:"System architecture patterns and integration strategies are informed by:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["ROS 2 Design Patterns: ",(0,s.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Concepts/About-Different-Middleware-Vendors.html",children:"ROS 2 Documentation - Design"})]}),"\n",(0,s.jsx)(n.li,{children:"State Machine Frameworks: SMACH (ROS 1), BehaviorTree.CPP (modern alternative)"}),"\n",(0,s.jsxs)(n.li,{children:["VLA System Architectures: ",(0,s.jsx)(n.a,{href:"https://robotics-transformer2.github.io/",children:"RT-2: Vision-Language-Action Models"})," (Google DeepMind, 2023)"]}),"\n",(0,s.jsxs)(n.li,{children:["Failure Recovery in Mobile Manipulation: ",(0,s.jsx)(n.a,{href:"https://ieeexplore.ieee.org/",children:"IEEE Paper on Robust Grasping"})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Full citation index: ",(0,s.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/appendices/citations",children:"Appendix D: Citations"})]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Building autonomous humanoid systems requires more than just capable individual components\u2014it demands thoughtful integration architecture. In this chapter, you learned:"}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"State machines"})," provide structured execution flow and failure handling for complex tasks"]}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"ROS 2 communication patterns"})," (topics, actions, services) enable distributed, asynchronous coordination"]}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Data flow diagrams"})," reveal bottlenecks and help debug multi-component systems"]}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Failure recovery strategies"})," (retry budgets, clarification requests) make systems robust to real-world uncertainty"]}),"\n",(0,s.jsx)(n.p,{children:"The architecture presented here scales beyond fetch-and-deliver tasks to general-purpose autonomous manipulation: from warehouse picking to surgical assistance."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next"}),": ",(0,s.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/chapter-02-voice-llm",children:"Chapter 2: Voice & LLM Pipeline"}),' dives into implementing the "brain" of the system\u2014natural language understanding and task planning.']}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(n.p,{children:["\u2b50 ",(0,s.jsx)(n.strong,{children:"Exercise 1"}),": Draw the complete state transition diagram showing all 11 states and transition conditions. Identify which transitions represent happy path vs. error handling."]}),"\n",(0,s.jsxs)(n.p,{children:["\u2b50\u2b50 ",(0,s.jsx)(n.strong,{children:"Exercise 2"}),": Modify the state machine to support ",(0,s.jsx)(n.strong,{children:"multi-object tasks"}),' (e.g., "Bring me the red cup and the blue plate"). What new states are needed? How does the LLM output format change?']}),"\n",(0,s.jsxs)(n.p,{children:["\u2b50\u2b50\u2b50 ",(0,s.jsx)(n.strong,{children:"Exercise 3"}),": Design a ",(0,s.jsx)(n.strong,{children:"behavior tree"})," alternative to the state machine. Compare expressiveness, readability, and ease of adding new capabilities (e.g., door opening, drawer manipulation)."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Word Count"}),": 381 words (Target: 350-400) \u2705"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var t=i(6540);const s={},r=t.createContext(s);function o(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);