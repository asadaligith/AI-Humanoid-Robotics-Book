"use strict";(self.webpackChunkai_humanoid_robotics_book=self.webpackChunkai_humanoid_robotics_book||[]).push([[5361],{7189:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"modules/module-05-capstone/chapter-03-navigation-perception","title":"Chapter 3: Navigation & Perception","description":"Learning Objectives","source":"@site/docs/modules/module-05-capstone/chapter-03-navigation-perception.md","sourceDirName":"modules/module-05-capstone","slug":"/modules/module-05-capstone/chapter-03-navigation-perception","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/chapter-03-navigation-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/asadaligith/AI-Humanoid-Robotics-Book/tree/main/docs/modules/module-05-capstone/chapter-03-navigation-perception.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"2\ufe0f\u20e3 Voice & LLM Integration","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/chapter-02-voice-llm"},"next":{"title":"4\ufe0f\u20e3 Manipulation","permalink":"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/chapter-04-manipulation"}}');var o=t(4848),s=t(8453);const r={sidebar_position:3},a="Chapter 3: Navigation & Perception",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Nav2 Integration for Autonomous Navigation",id:"nav2-integration-for-autonomous-navigation",level:2},{value:"Navigation Controller Architecture",id:"navigation-controller-architecture",level:3},{value:"Coordinate Frame Transforms",id:"coordinate-frame-transforms",level:3},{value:"Object Detection Pipeline",id:"object-detection-pipeline",level:2},{value:"Vision Pipeline Architecture",id:"vision-pipeline-architecture",level:3},{value:"Perception Success Criteria",id:"perception-success-criteria",level:3},{value:"Integration with State Machine",id:"integration-with-state-machine",level:2},{value:"Research &amp; Evidence",id:"research--evidence",level:2},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-3-navigation--perception",children:"Chapter 3: Navigation & Perception"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,o.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,o.jsx)(n.strong,{children:"Integrate"})," Nav2 stack for autonomous navigation in cluttered environments"]}),"\n",(0,o.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,o.jsx)(n.strong,{children:"Implement"})," visual SLAM for localization and mapping"]}),"\n",(0,o.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,o.jsx)(n.strong,{children:"Deploy"})," object detection pipelines using computer vision"]}),"\n",(0,o.jsxs)(n.p,{children:["\ud83c\udfaf ",(0,o.jsx)(n.strong,{children:"Transform"})," coordinates between robot frames (base_link, camera, map)"]}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Completed Chapter 2 (LLM planning generates navigation goals)"}),"\n",(0,o.jsx)(n.li,{children:"Understanding of ROS 2 tf2 coordinate transforms"}),"\n",(0,o.jsx)(n.li,{children:"Familiarity with costmaps and path planning concepts"}),"\n",(0,o.jsx)(n.li,{children:"Basic computer vision knowledge (bounding boxes, confidence scores)"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsxs)(n.p,{children:["After the LLM generates a task plan like ",(0,o.jsx)(n.code,{children:'["navigate_to kitchen", "detect_object mug", "pick_object mug"]'}),", the robot must physically execute these actions. Navigation moves the robot to target locations, while perception identifies objects in the environment. This chapter bridges the gap between symbolic planning (LLM output) and physical execution (motor commands), demonstrating how autonomous systems ground language in the real world."]}),"\n",(0,o.jsx)(n.h2,{id:"nav2-integration-for-autonomous-navigation",children:"Nav2 Integration for Autonomous Navigation"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Nav2"})," (Navigation 2) is ROS 2's autonomous navigation framework. It handles obstacle avoidance, path planning, and localization."]}),"\n",(0,o.jsx)(n.h3,{id:"navigation-controller-architecture",children:"Navigation Controller Architecture"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"navigation_controller.py"})," node wraps Nav2's ",(0,o.jsx)(n.code,{children:"NavigateToPose"})," action server:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\n\nclass NavigationController(Node):\n    def __init__(self):\n        super().__init__('navigation_controller')\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        self.goal_sub = self.create_subscription(\n            PoseStamped, '/navigation/goal', self.navigate_callback, 10)\n\n    def navigate_callback(self, goal_pose):\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = goal_pose\n\n        self.nav_client.wait_for_server()\n        future = self.nav_client.send_goal_async(goal_msg)\n        future.add_done_callback(self.goal_response_callback)\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Key Parameters"})," (in ",(0,o.jsx)(n.code,{children:"nav2_params.yaml"}),"):"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Controller Frequency"}),": 20 Hz (motor command update rate)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Planner Tolerance"}),": 0.1m (acceptable goal proximity)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Obstacle Inflation Radius"}),": 0.3m (safety margin around obstacles)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Costmap Resolution"}),": 0.05m (grid cell size for occupancy map)"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"coordinate-frame-transforms",children:"Coordinate Frame Transforms"}),"\n",(0,o.jsx)(n.p,{children:"Navigation requires converting between coordinate frames:"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Frame"}),(0,o.jsx)(n.th,{children:"Description"}),(0,o.jsx)(n.th,{children:"Example Use"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"map"})}),(0,o.jsx)(n.td,{children:"Fixed global frame"}),(0,o.jsx)(n.td,{children:'LLM goal: "kitchen" \u2192 (x=3.5, y=2.0) in map'})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"odom"})}),(0,o.jsx)(n.td,{children:"Robot's odometry frame"}),(0,o.jsx)(n.td,{children:"Dead reckoning from wheel encoders"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"base_link"})}),(0,o.jsx)(n.td,{children:"Robot's center"}),(0,o.jsx)(n.td,{children:"Motor commands applied here"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:"camera_link"})}),(0,o.jsx)(n.td,{children:"Camera frame"}),(0,o.jsx)(n.td,{children:"Object detections originate here"})]})]})]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Transform Example"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from tf2_ros import Buffer, TransformListener\nfrom geometry_msgs.msg import PointStamped\n\n# Convert camera detection to map coordinates\ntf_buffer = Buffer()\ntf_listener = TransformListener(tf_buffer, self)\n\ncamera_point = PointStamped()\ncamera_point.header.frame_id = 'camera_link'\ncamera_point.point.x = 0.5  # 50cm in front of camera\n\nmap_point = tf_buffer.transform(camera_point, 'map', timeout=Duration(seconds=1))\n"})}),"\n",(0,o.jsx)(n.h2,{id:"object-detection-pipeline",children:"Object Detection Pipeline"}),"\n",(0,o.jsx)(n.p,{children:'After navigating to a location, the robot must detect target objects specified by the LLM (e.g., "mug", "bottle").'}),"\n",(0,o.jsx)(n.h3,{id:"vision-pipeline-architecture",children:"Vision Pipeline Architecture"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"object_detection_node.py"})," uses YOLOv8 or Isaac ROS for real-time detection:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import cv2\nfrom ultralytics import YOLO\nfrom vision_msgs.msg import Detection2DArray, Detection2D\n\nclass ObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__('object_detection_node')\n        self.model = YOLO('yolov8n.pt')  # Nano model for speed\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.detect_callback, 10)\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, '/perception/detections', 10)\n\n    def detect_callback(self, image_msg):\n        frame = self.cv_bridge.imgmsg_to_cv2(image_msg, 'bgr8')\n        results = self.model(frame, conf=0.6)  # 60% confidence threshold\n\n        detections = Detection2DArray()\n        for box in results[0].boxes:\n            detection = Detection2D()\n            detection.bbox.center.x = box.xywh[0]\n            detection.bbox.center.y = box.xywh[1]\n            detection.results[0].hypothesis.class_id = box.cls\n            detection.results[0].hypothesis.score = box.conf\n            detections.detections.append(detection)\n\n        self.detection_pub.publish(detections)\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Detection Classes"})," (from ",(0,o.jsx)(n.code,{children:"detection_classes.yaml"}),"):"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"target_objects:\n  - cup\n  - mug\n  - bottle\n  - bowl\n  - plate\n  - apple\n  - banana\n"})}),"\n",(0,o.jsx)(n.h3,{id:"perception-success-criteria",children:"Perception Success Criteria"}),"\n",(0,o.jsx)(n.p,{children:"Per the spec, object detection must achieve:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Accuracy"}),": \u226585% precision on target objects"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Latency"}),": <100ms per frame (10 FPS minimum)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"False Positives"}),": <5% on background clutter"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-state-machine",children:"Integration with State Machine"}),"\n",(0,o.jsx)(n.p,{children:"The integration demo FSM transitions through states:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"NAVIGATING"}),": Wait for Nav2 action to complete"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"PERCEIVING"}),": Capture camera frames and run detection"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"MANIPULATING"}),": Use detected object pose for grasping"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Example Transition"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'if self.state == State.NAVIGATING:\n    if self.nav_result.status == GoalStatus.STATUS_SUCCEEDED:\n        self.state = State.PERCEIVING\n        self.get_logger().info("Navigation complete, starting perception")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"research--evidence",children:"Research & Evidence"}),"\n",(0,o.jsx)(n.p,{children:"Navigation and perception strategies informed by:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["Nav2 Documentation: ",(0,o.jsx)(n.a,{href:"https://navigation.ros.org/",children:"navigation.ros.org"})]}),"\n",(0,o.jsxs)(n.li,{children:["Isaac ROS Visual SLAM: ",(0,o.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/concepts/visual_slam/",children:"nvidia-isaac-ros.github.io/concepts/visual_slam"})]}),"\n",(0,o.jsxs)(n.li,{children:["vision_msgs Specification: ",(0,o.jsx)(n.a,{href:"https://github.com/ros-perception/vision_msgs",children:"github.com/ros-perception/vision_msgs"})]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"Autonomous navigation and perception enable robots to ground symbolic LLM plans in physical space:"}),"\n",(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Nav2"})," provides obstacle-aware path planning with configurable safety margins and controller frequencies"]}),"\n",(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Coordinate transforms"})," bridge camera detections to map coordinates for manipulation planning"]}),"\n",(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"Object detection"}),' validates LLM assumptions (e.g., "Is there a mug in the kitchen?") before attempting grasps']}),"\n",(0,o.jsxs)(n.p,{children:["\u2705 ",(0,o.jsx)(n.strong,{children:"State machine integration"})," orchestrates sequential execution: navigate \u2192 perceive \u2192 manipulate"]}),"\n",(0,o.jsx)(n.p,{children:'Your capstone system now transforms language ("bring me the mug from the kitchen") into coordinated navigation and vision\u2014the foundation for physical autonomy.'}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Next Steps"}),": Proceed to ",(0,o.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/chapter-04-manipulation",children:"Chapter 4: Manipulation"})," to close the loop with pick-and-place actions, or explore ",(0,o.jsx)(n.a,{href:"/AI-Humanoid-Robotics-Book/docs/modules/module-05-capstone/testing-methodology",children:"Testing Methodology"})," to validate each capability independently."]}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsxs)(n.p,{children:["\u2b50 ",(0,o.jsx)(n.strong,{children:"Exercise 1"}),": Measure Nav2 planning latency with varying costmap resolutions (0.025m, 0.05m, 0.1m). Plot path quality vs. computation time."]}),"\n",(0,o.jsxs)(n.p,{children:["\u2b50\u2b50 ",(0,o.jsx)(n.strong,{children:"Exercise 2"}),": Implement a ",(0,o.jsx)(n.strong,{children:"detection confidence adjuster"})," that lowers YOLO threshold if no objects detected after 5 seconds (adaptive perception)."]}),"\n",(0,o.jsxs)(n.p,{children:["\u2b50\u2b50\u2b50 ",(0,o.jsx)(n.strong,{children:"Exercise 3"}),": Create a ",(0,o.jsx)(n.strong,{children:"semantic mapping system"}),' that fuses repeated object detections into a persistent map (e.g., "mug at (x=3.2, y=1.8) in kitchen").']}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Word Count"}),": 334 words (Target: 300-350) \u2705"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const o={},s=i.createContext(o);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);