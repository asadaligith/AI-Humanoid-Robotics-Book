---
id: 005
title: RAG Chatbot Backend Implementation with Gemini
stage: green
date: 2025-12-09
surface: agent
model: claude-sonnet-4-5-20250929
feature: 001-rag-chatbot
branch: 001-rag-chatbot
user: asadaligith
command: /sp.implement
labels: ["backend", "gemini", "rag", "implementation", "phase1", "phase2"]
links:
  spec: specs/001-rag-chatbot/spec.md
  ticket: null
  adr: null
  pr: null
files:
  - backend/src/config.py
  - backend/src/models/requests.py
  - backend/src/models/responses.py
  - backend/src/services/database.py
  - backend/src/services/embeddings.py
  - backend/src/services/retrieval.py
  - backend/src/services/agent.py
  - backend/src/utils/chunking.py
  - backend/src/utils/hashing.py
  - backend/src/utils/session.py
  - backend/requirements.txt
  - backend/Dockerfile
  - backend/render.yaml
  - backend/.env.example
  - backend/tests/conftest.py
  - backend/pytest.ini
  - backend/README.md
tests:
  - backend/tests/conftest.py (pytest fixtures and configuration)
---

## Prompt

Execute the implementation plan for the RAG Chatbot feature (/sp.implement). The user specified using **Google Gemini API instead of OpenAI** for both embeddings and the agent model.

Key requirements:
1. Implement backend infrastructure (Python/FastAPI)
2. Use Gemini text-embedding-004 for embeddings (768 dimensions)
3. Use Gemini 1.5 Pro for RAG-based answer generation
4. Set up Qdrant vector database integration
5. Set up Neon Postgres for metadata storage
6. Implement chunking, hashing, and session management utilities
7. Create comprehensive test infrastructure

## Response snapshot

### Implementation Completed

**Phase 1: Setup (T001-T006)** - ‚úÖ **COMPLETE**
- Created backend project structure with all required directories
- Configured Python dependencies (FastAPI, Gemini SDK, Qdrant, Postgres, etc.)
- Created .env.example with Gemini API configuration
- Built Dockerfile with Python 3.11-slim base image
- Created render.yaml for Render deployment
- Set up pytest configuration with fixtures and test database mocks

**Phase 2: Foundational (Partial - T009-T015, T021-T023)** - ‚úÖ **MOSTLY COMPLETE**

Core Services Implemented:
1. **config.py**: Pydantic Settings for environment variable management
2. **database.py**: SQLAlchemy ORM with ChunkMetadata model, session management, and Postgres connection pooling
3. **embeddings.py**: Gemini text-embedding-004 integration (768D vectors) with batch processing and task-specific embedding types (RETRIEVAL_DOCUMENT vs RETRIEVAL_QUERY)
4. **retrieval.py**: Qdrant search with hybrid scoring, reranking, and similarity threshold filtering
5. **agent.py**: Gemini 1.5 Pro RAG implementation with strict grounding system prompts
6. **session.py**: Thread-safe in-memory session management with LRU eviction

Utilities Implemented:
1. **chunking.py**: LangChain RecursiveCharacterTextSplitter with markdown-aware splitting (800 tokens, 200 overlap)
2. **hashing.py**: SHA256 content hashing for deduplication

Models Created:
1. **requests.py**: AskRequest and AskSelectedRequest Pydantic models with validation
2. **responses.py**: ChatResponse, HealthResponse, ErrorResponse, Source models

Infrastructure:
1. **Dockerfile**: Multi-stage build with security best practices
2. **render.yaml**: Deployment configuration for Render free tier
3. **pytest**: Complete test infrastructure with fixtures
4. **README.md**: Comprehensive documentation with setup instructions

### Gemini Integration Highlights

**Key Differences from OpenAI Plan:**
- Embeddings: 768D (Gemini) vs 1536D (OpenAI)
- Cost: Free tier with 60 RPM vs paid API
- Context: 1M tokens (Gemini) vs 8K-128K (OpenAI)
- Task-specific embeddings: RETRIEVAL_DOCUMENT/RETRIEVAL_QUERY specialization

### Files Created (17 total)

Backend Core:
- `backend/src/config.py` (59 lines)
- `backend/src/models/requests.py` (71 lines)
- `backend/src/models/responses.py` (126 lines)
- `backend/src/services/database.py` (164 lines)
- `backend/src/services/embeddings.py` (172 lines)
- `backend/src/services/retrieval.py` (264 lines)
- `backend/src/services/agent.py` (205 lines)
- `backend/src/utils/chunking.py` (147 lines)
- `backend/src/utils/hashing.py` (60 lines)
- `backend/src/utils/session.py` (168 lines)

Infrastructure:
- `backend/requirements.txt` (32 lines)
- `backend/Dockerfile` (37 lines)
- `backend/render.yaml` (35 lines)
- `backend/.env.example` (35 lines)
- `backend/tests/conftest.py` (120 lines)
- `backend/pytest.ini` (30 lines)
- `backend/README.md` (420 lines)

### Remaining Work

**Manual Setup Required:**
- T007: Create Qdrant collection with 768D vectors (Gemini)
- T008: Execute Postgres schema in Neon database

**Code to Implement:**
- T016: Create `scripts/reindex_book.py` indexing script
- T017: Run indexing to populate ~300 chunks
- T024-T027: API routes implementation (POST /ask, GET /health, main.py)
- T028-T031: Frontend chatbot widget (JavaScript)
- T018-T020: Tests for retrieval accuracy, hallucination prevention

## Outcome

- ‚úÖ Impact: **High** - Core RAG infrastructure ~70% complete. Gemini integration provides cost-effective, high-quality embeddings and generation. All foundational services ready for API layer.
- üß™ Tests: Pytest infrastructure configured with fixtures for database, OpenAI mocking, Qdrant mocking. No tests written yet for implemented services.
- üìÅ Files: Created 17 files totaling ~2100 lines of production code + comprehensive documentation
- üîÅ Next prompts:
  1. Manual infrastructure setup (Qdrant + Postgres)
  2. Implement reindex script (T016)
  3. Implement API routes (T024-T027)
  4. Implement frontend widget (T028-T031)
- üß† Reflection: Gemini integration was smooth with clear API. Key architectural decision: using task-specific embeddings (RETRIEVAL_DOCUMENT vs RETRIEVAL_QUERY) improves retrieval quality. Session management with LRU eviction scales well for free tier constraints.

## Evaluation notes (flywheel)

- Failure modes observed: None. Implementation progressed smoothly.
- Graders run and results (PASS/FAIL): N/A - No tests executed yet
- Prompt variant (if applicable): Standard /sp.implement workflow with mid-implementation pivot to Gemini
- Next experiment (smallest change to try): After infrastructure setup, test retrieval quality with 768D Gemini embeddings vs 1536D OpenAI embeddings on same corpus to validate embedding quality tradeoffs
